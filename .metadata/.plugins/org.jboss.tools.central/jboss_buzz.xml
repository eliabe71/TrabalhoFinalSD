<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Keycloak 17.0.0 released</title><link rel="alternate" href="https://www.keycloak.org/2022/02/keycloak-1700-released" /><author><name /></author><id>https://www.keycloak.org/2022/02/keycloak-1700-released</id><updated>2022-02-11T00:00:00Z</updated><content type="html">To download the release go to . RELEASE NOTES HIGHLIGHTS QUARKUS DISTRIBUTION IS NOW FULLY SUPPORTED The default Keycloak distribution is now based on Quarkus. The new distribution is faster, leaner, and a lot easier to configure! We appreciate migrating from the WildFly distribution is not going to be straightforward for everyone, since how you start and configure Keycloak has radically changed. With that in mind we will continue to support the WildFly distribution until June 2022. For information on how to migrate to the new distribution check out the migration/migrating-to-quarkus[Quarkus Migration Guide]. QUARKUS DISTRIBUTION UPDATES A lot of effort went into polishing and improving the Quarkus distribution to make it as good as an experience as possible. A few highlights include: * A new approach to documentation in form of server guides to help you install and configure Keycloak * Upgraded Quarkus to 2.7.0.Final * Configuration file is on longer Java specific, and aligns configuration keys with CLI arguments * Clearer separation between build options and runtime configuration. * h2-mem and h2-file databases renamed to dev-mem and dev-file. * Simplified enabling and disabling features * Custom, and unsupported, Quarkus configuration is done through conf/quarkus.properties. * Ability to add custom Java Options via JAVA_OPTS_APPEND (thanks to ) * Initial logging capabilities * Initial support for Cross-DC * User-defined profiles are no longer supported but using different configuration files to achieve the same goal * Quickstarts updated to use the new distribution == Other improvements OFFLINE SESSIONS LAZY LOADED The offline sessions are now lazily fetched from the database by default instead of preloading during the server startup. To change the default behavior, see . IMPROVED USER SEARCH Keycloak now supports a glob-like syntax for the user search when listing users in the Admin Console, which allows for three different types of searches: prefix (foo* which became the default search), infix (*foo*), and exact "foo") MIGRATION FROM 16.1 Before you upgrade remember to backup your database. If you are not on the previous release refer to for a complete list of migration changes. DEFAULT DISTRIBUTION IS NOW POWERED BY QUARKUS The default distribution of Keycloak is now powered by Quarkus, which brings a number of breaking changes to you configure Keycloak and deploy custom providers. For more information check out the migration/migrating-to-quarkus[Quarkus Migration Guide]. The WildFly distribution of Keycloak is now deprecated, with support ending June 2022. We recommend migrating to the Quarkus distribution as soon as possible. However, if you need to remain on the legacy WildFly distribution for some time, there are some changes to consider: * Container images for the legacy distribution tags have changed. To use the legacy distribution use the tags legacy or 17.0.0-legacy. * Download on the website for the legacy distribution has changed to keycloak-legacy-17.0.0.[zip|tar.gz]. If you encounter problems migrating to the Quarkus distribution, missing ability to configure something, or have general ideas and feedback, please open a discussion in . MIGRATING FROM THE PREVIEW QUARKUS DISTRIBUTION A number of things have changed since the preview Quarkus distribution was released in Keycloak 15.1.0. The ideal way to learn about what’s changed is to check out the new . In summary, the changes include: * Container now published to quay.io/keycloak/keycloak:latest and quay.io/keycloak/keycloak:17.0.0 * Download on website renamed to keycloak-17.0.0.[zip|tar.gz]. * conf/keycloak.properties changed to conf/keycloak.conf, which unifies configuration keys between the config file and CLI arguments. * Clearer separation between build options and runtime configuration. * Custom Quarkus configuration is done through conf/quarkus.properties. * h2-mem and h2-file databases renamed to dev-mem and dev-file. * Features are now enabled/disabled with --features and --features-disabled replacing the previous approach that had an separate config key for each feature. * Runtime configuration can no longer be passed to kc.[sh|bat] build and is no longer persisted in the build * Logging level and format is now configured with --log-level and --log-format, while in the past these had to be configured using unsupported Quarkus properties. CLIENT POLICIES MIGRATION : CLIENT-SCOPES If you used a policy including client-scopes condition and edited JSON document directly, you will need to change the "scope" field name in a JSON document to "scopes". LIQUIBASE UPGRADED TO VERSION 4.6.2 Liquibase was updated from version 3.5.5 to 4.6.2, which includes, among other things, several bug fixes, and a new way of registering custom extensions using ServiceLoader. Migration from previous Keycloak versions to Keycloak 17.0.0 has been extensively tested with all currently supported databases, but we would like to stress the importance of closely following the , specifically of backing up existing database before upgrade. While we did our best to test the consequences of the Liquibase upgrade, some installations could be using specific setup unknown to us. ALL RESOLVED ISSUES NEW FEATURES * Convert MapUserEntity to interface keycloak storage * Create Operator.X module in repo keycloak operator * Keycloak.X deployment keycloak operator * Realm CRD keycloak operator * Testsuite baseline keycloak operator * Let users configure Dynamic Client Scopes keycloak * Create an internal representation of RAR that also handles Static and Dynamic Client Scopes keycloak * Handle Dynamic Scopes correctly in the consent screen keycloak * Publish ECMAScript Modules for keycloak-js keycloak adapter/javascript * Package server guides to be used in the website keycloak dist/quarkus * JPA map storage: Client scope no-downtime store keycloak storage * Convert authorization services entities into interface keycloak storage * Generate the CRD from RealmRepresentation keycloak operator * Improve user search query keycloak storage * Configurable session limits keycloak authentication ENHANCEMENTS * Update Kubernetes and OpenShift examples used by getting started guides to use Quarkus dist keycloak-quickstarts * Update getting-started in QuickStarts to use Quarkus dist keycloak-quickstarts * Update default container to Quarkus keycloak-containers * Update documentation for Quarkus distribution keycloak-documentation * Release notes for Keycloak 17 keycloak-documentation * Migration from Keycloak.X preview keycloak-documentation * Documentation for Quarkus distribution keycloak dist/quarkus * Disable pre-loading offline sessions by default keycloak dist/quarkus * Remove Hashicorp Support keycloak dist/quarkus * Make JsonbType generic keycloak storage * Tree storage: introduce notion of per-field primary and cached status in an entity keycloak storage * Provide documentation for proxy mode in Quarkus based Keycloak keycloak dist/quarkus * Review README files. keycloak dist/quarkus * Add indexing to HotRodGroupEntity keycloak storage * Make JpaClientStorage* classes generic keycloak storage * Refactor generated constructors in new store entities keycloak storage * Avoid building configuration all the time when running tests keycloak * Remove override on xmlsec in quarkus/pom.xml keycloak dist/quarkus * Database configuration tests keycloak * Upgrade Infinispan to 12.1.7.Final keycloak storage * Cross-site validation for lazy loading of offline sessions keycloak storage * Switch default offline sessions to lazy loaded keycloak docs * HotRod map storage uses regex pattern that could be precompiled keycloak storage * Add configuration guide keycloak dist/quarkus * Validation for CIBA binding_message parameter keycloak * Upgrade Liquibase to 4.6.2 keycloak storage * Add more details about 2FA to authenticate page keycloak * Verify the WebAuthn functionality and settings for authentication keycloak testsuite * Enable only TLSv1.3 as default for the https protocol and set expected values keycloak dist/quarkus * Backward compatibility for lower-case bearer type in token responses keycloak * Test scenarios for verifying of JS injection for WebAuthn Policy keycloak testsuite * Improve the kustomize setup for the operator keycloak operator * Readiness and Liveness probe for the operator deployment of Keycloak.X keycloak operator * Multiple warnings caused by typed varargs in TokenVerifier keycloak * optimize title/summary and headlines for proxy guide keycloak dist/quarkus * Add support to linking between guides keycloak docs * Remove output of summary in guides keycloak docs * Build command should only accept built-time options keycloak dist/quarkus * Exclude some folders from our SAST analysis keycloak * Convert MapClientScopeEntity to interface keycloak storage * Add a quarkus.properties for unsupported configuration options keycloak dist/quarkus * Remove any reference to configuration profile keycloak dist/quarkus * Remove system property from help message keycloak dist/quarkus * Hide Hasicorp Vault from CLI keycloak dist/quarkus * Improve enabling/disabling features in Quarkus distribution keycloak dist/quarkus * Device Authorization Grant with PKCE keycloak * Adpaters for Map Storage swallow multi-valued attribute while Keycloak Core doesn't support them keycloak storage * Restrict Dynamic Scopes to optional Client Scopes keycloak * Add section recommended exposed paths to reverse proxy documentation keycloak docs * Combine package files for JS adapter keycloak adapter/javascript * Add test scenarios for Passwordless Webauthn AIA keycloak testsuite * Extend and fix tests for Resident Keys for WebAuthn keycloak testsuite * Store information about transport media of WebAuthn authenticator keycloak authentication/webauthn * Sort options in guides by key keycloak docs * Update default ZIP distribution to Quarkus keycloak dist/wildfly * Complete support for Passwordless tests keycloak testsuite * Use keycloak.v2 admin theme by default if admin2 is enabled keycloak admin/ui * Quarkus update to 2.7.0 Final keycloak dist/quarkus * Initial logging support keycloak dist/quarkus * Add support for pinning guides to the top keycloak docs * Implement the Dynamic Scopes parsing for the resource-owner-password-credentials grant. keycloak oidc * Verify if enabling authentication and encryption for JGroups work on Quarkus dist keycloak dist/quarkus * Add note about escaping of vaules for config keycloak dist/quarkus * Logging guide for Quarkus dist keycloak dist/quarkus * Update com.github.ua-parser:uap-java to 1.5.2 keycloak dependencies * Remove external Collection utility class for WebAuthn keycloak authentication/webauthn * Cover enabling mtls in TLS guide keycloak dist/quarkus * Updated use of generics in JPA Map Storage keycloak storage * Create common parent for Jpa*AttributeEntity keycloak storage * Reduce Keycloak.x image size keycloak dist/quarkus BUGS * Incorrect dependency in package.json keycloak-nodejs-connect * KeycloakAuthenticatorValve (Tomcat) does not implement createRequestAuthenticator() keycloak adapter/jee * Spurious logs are spilling in Quarkus Distribution.X integration tests keycloak dist/quarkus * The title of the login screen is not translated into Japanese keycloak * Quarkus relational database setup documentation error keycloak * "look-ahead window" of TOTP should be "look around window" keycloak * Expected Scopes of ClientScopesCondition created on Admin UI are not saved onto ClientScopesCondition.Configuration keycloak authorization-services * Password credential decoding from DB may fail in rare cases - No login possible keycloak * Dist.X cannot connect to external Postgres if the password ends with an = sign keycloak * Dist.X apparently doesn't apply correctly the db schema keycloak * JPA-Map storage might loose writes due to missing locking mechanism keycloak storage * Multiple active tabs when realm name equals name of tab in Admin console keycloak admin/ui * Missing german translation for webauthn-doAuthenticate keycloak translations * Hard coded message within account console v2 keycloak account/ui * Client Policies : Condition's negative logic configuration is not shown in Admin Console's form view keycloak * Placeholders in keycloak.properties do not get substituted at runtime after a build keycloak * Keycloak Server throws NPE at startup when the MAP_STORAGE feature is enabled keycloak storage * Setting "24 mins" to timeout, the admin console displays "1 day" keycloak admin/ui * Username editable when user is forced to re-authenticate keycloak authentication * Quarkus dist "providers" dir has outdated README keycloak dist/quarkus * Newline in localization messages causes uncaught syntax error in account console v2 keycloak account/ui * Dist.X argument parsing fails on semicolon keycloak dist/quarkus * KEYCLOAK-19289 check if values to set is not null keycloak * LDAP connection timeout is treated as login failure and brute force locking the user keycloak * Different method getGroupsCountByNameContaining in MapGroupProvider and JpaRealmProvider keycloak storage * MapRoleProvider could return also client roles when searching for realm roles keycloak storage * Missing DB constraints for JPA Map Storage for Clients keycloak storage * Scope bug in device authorization request keycloak * Handling lazy loading exceptions for storage in new and old storage keycloak storage * Model tests consistently time out keycloak storage * Keycloak.X cannot lookup embedded theme-resources from extension jars keycloak dist/quarkus * WebAuthnSigningInTest failures in pipeline keycloak testsuite * GHA failing due to wrong scheme when downloading ISPN server keycloak * Fixes for token revocation keycloak oidc * JPA Map storage doesn't downgrade entityVersion when modifying a row written with a future entityVersion keycloak storage * Updated flag disappearing for nested entities in HotRod store keycloak storage * Build command exits with success with invalid arguments keycloak dist/quarkus * Review guides to use the correct format for options keycloak docs * Mapped Quarkus properties should not be persisted keycloak dist/quarkus * Unstable model tests when starting multiple Infinispan instances keycloak storage * JPA Map storage doesn't increment version column on attribute update keycloak storage * Update Portuguese (Brazil) translations keycloak translations * Do not run re-augmentation if config is the same in dev mode keycloak dist/quarkus * Errors from CLI are masked by help keycloak dist/quarkus * Rename h2-file/h2-mem to dev-file/dev-mem and remove default values for username/password keycloak dist/quarkus * Keycloak is not capturing proper Signing details(like browser name and version) when logged in from different browsers keycloak account/ui * Not possible to register webauthn key on Firefox keycloak authentication/webauthn * JPA Map storage listener should handle optimistic locking for deleting entities keycloak storage * Failing to use cache remote-stores due to missing dependencies keycloak dist/quarkus * Can not set a jgroups stack other than the defaults from Infinispan keycloak dist/quarkus * JPA delegates can throw NoResultException when entity doesn't have any attributes keycloak storage UPGRADING Before you upgrade remember to backup your database and check the for anything that may have changed.</content><dc:creator /></entry><entry><title type="html">JSON DataSets in Dashbuilder</title><link rel="alternate" href="https://blog.kie.org/2022/02/json-datasets-in-dashbuilder.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2022/02/json-datasets-in-dashbuilder.html</id><updated>2022-02-10T16:47:16Z</updated><content type="html">Datasets providers run on the same process as Dashbuilder, which means that if you only use a specific provider, all the others are idle, as part of Dashbuilder. In addition to that, we need to maintain all providers.  In the most recent release a new dataset provider type was added: External Data Sets. With External DataSets any JSON array can be a dataset. Simplest dataset Dashbuilder tries to guess the column type (LABEL, NUMBER, DATE, default type is label) and gives it a generic name. Users that want to force a type can use a more complex JSON to include column information: Sample dataset with columns information The source for the JSON can be an HTTP URL or a file URL, meaning that any JSON on the WEB can be used as a dataset.  The URL is the only information that the user must provide about the dataset and the whole dataset is loaded into memory each time it is accessed. If the source JSON is not changed then users can set up a cache for the dataset.  Bear in mind that only the “Data refresh every” is used by the external provider. The URL for the dataset can be changed at runtime using Java System property dashbuilder.dataset.%s.url – replace %s with the dataset name – to point to a new dataset JSON. Users can add more rows to this new definition or change the existing rows, but do not change columns definition (type and ID) or it will result in an expected error in the reports that use the dataset. FUTURE Client Loading A new feature to call datasets from the client side is in development. It will only support  HTTP URLs and users must ensure that CORS is enabled for the dashbuilder domain. Dynamic Lookup Currently the whole dataset is retrieved from the source. A new feature will allow users to receive the front end request (lookup) to retrieve the filtered dataset. The post appeared first on .</content><dc:creator>William Siqueira</dc:creator></entry><entry><title>Kafka Monthly Digest: January 2022</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/02/10/kafka-monthly-digest-january-2022" /><author><name>Mickael Maison</name></author><id>ded30a3f-38d2-4493-ad49-5a070aef3af2</id><updated>2022-02-10T07:00:00Z</updated><published>2022-02-10T07:00:00Z</published><summary type="html">&lt;p&gt;This 48th edition of the Kafka Monthly Digest covers what happened in the &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; community in January 2022. The release of Apache Kafka 3.1.0 is the big news, including updates to Kafka Connect and Kafka Streams. I'll also discuss new KIPs and &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; releases in January 2022.&lt;/p&gt; &lt;p&gt;This is the fourth anniversary of this series! If you are interested in the history behind the Kafka Monthly Digest, last year I explained on &lt;a href="https://twitter.com/MickaelMaison/status/1355930398677757955"&gt;Twitter&lt;/a&gt; how I got started.&lt;/p&gt; &lt;p&gt;For last month's digest, see &lt;a href="https://developers.redhat.com/articles/2022/01/11/kafka-monthly-digest-december-2021"&gt;Kafka Monthly Digest: December 2021&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Kafka 3.1.0 release&lt;/h2&gt; &lt;p&gt;David Jacot released Kafka 3.1.0 on January 24 and published an announcement on the &lt;a href="https://blogs.apache.org/kafka/entry/what-s-new-in-apache7"&gt;Apache blog&lt;/a&gt;. As always, you can find the complete list of changes in the &lt;a href="https://www.apache.org/dist/kafka/3.1.0/RELEASE_NOTES.html"&gt;release notes&lt;/a&gt; or the &lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/Release+Plan+3.1.0"&gt;release plan&lt;/a&gt; on the Kafka wiki.&lt;/p&gt; &lt;p&gt;Kafka 3.1 adds support for Java 17. Note that running Kafka in &lt;a href="https://github.com/apache/kafka/blob/trunk/config/kraft/README.md"&gt;KRaft mode&lt;/a&gt; (without ZooKeeper) is still not ready for production.&lt;/p&gt; &lt;p&gt;This new minor release brings a few interesting features, which I'll highlight in the next sections.&lt;/p&gt; &lt;h4&gt;Kafka brokers and clients&lt;/h4&gt; &lt;p&gt;Updates to the Kafka broker and clients include the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A production-ready OAuth implementation with OpenID Connect (OIDC) support (&lt;a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=186877575"&gt;KIP-768&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Metrics improvements include new metrics for active and fenced brokers (&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-748%3A+Add+Broker+Count+Metrics"&gt;KIP-748&lt;/a&gt;) and consistent names for latency metrics (&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-773%3A+Differentiate+consistently+metric+latency+measured+in+millis+and+nanos"&gt;KIP-773&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Topic IDs that were introduced in Kafka 2.8 are now used in &lt;code&gt;FetchRequests&lt;/code&gt; (&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-516%3A+Topic+Identifiers"&gt;KIP-516&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt;&lt;h4&gt;Kafka Connect&lt;/h4&gt; &lt;p&gt;The &lt;a href="https://kafka.apache.org/31/javadoc/org/apache/kafka/connect/mirror/ReplicationPolicy.html"&gt;ReplicationPolicy&lt;/a&gt; interface allows you to customize the names of MirrorMaker internal topics (&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-690%3A+Add+additional+configuration+to+control+MirrorMaker+2+internal+topics+naming+convention"&gt;KIP-690&lt;/a&gt;).&lt;/p&gt; &lt;h4&gt;Kafka Streams&lt;/h4&gt; &lt;p&gt;Updates to Kafka Streams include the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Foreign-key joins can now be performed using custom partitioners (&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-775%3A+Custom+partitioners+in+foreign+key+joins"&gt;KIP-775&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Querying &lt;a href="https://kafka.apache.org/31/javadoc/org/apache/kafka/streams/state/ReadOnlySessionStore.html"&gt;ReadOnlySessionStore&lt;/a&gt;, &lt;a href="https://kafka.apache.org/31/javadoc/org/apache/kafka/streams/state/ReadOnlyWindowStore.html"&gt;ReadOnlyWindowStore&lt;/a&gt;, and &lt;a href="https://kafka.apache.org/31/javadoc/org/apache/kafka/streams/state/ReadOnlyKeyValueStore.html"&gt;ReadOnlyKeyValueStore&lt;/a&gt; with unbounded ranges is now supported (&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-763%3A+Range+queries+with+open+endpoints"&gt;KIP-763&lt;/a&gt; and &lt;a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=186876596"&gt;KIP-766&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;The eager rebalance protocol is now deprecated. The new protocol, cooperative rebalancing, has been the default since Kafka 2.2. Eager will be removed in the next major release (&lt;a href="https://issues.apache.org/jira/browse/KAFKA-13439"&gt;KAFKA-13439&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;All uncaught exceptions are now wrapped into &lt;a href="https://kafka.apache.org/31/javadoc/org/apache/kafka/streams/errors/StreamsException.html"&gt;StreamsException&lt;/a&gt; (&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-783%3A+Add+TaskId+field+to+StreamsException"&gt;KIP-783&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Kafka Improvement Proposals&lt;/h2&gt; &lt;p&gt;Last month, the community submitted seven &lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals"&gt;KIPs&lt;/a&gt; (KIP-812 to KIP-818). I'll highlight just a few of them:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-813%3A+Shareable+State+Stores"&gt;KIP-813: Shareable state stores&lt;/a&gt;: This KIP proposes a mechanism for reusing existing Kafka Streams state stores in other Streams applications. In scenarios where multiple applications perform similar processing steps, this could be useful to avoid doing the same processing twice.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-814%3A+Static+membership+protocol+should+let+the+leader+skip+assignment"&gt;KIP-814: Static membership protocol should let the leader skip assignment&lt;/a&gt;: This KIP's goal is to improve support for static consumer group membership (introduced in Kafka 2.4). Today, in some cases, consumer groups with static membership can end up without a leader and miss metadata updates.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-817%3A+Fix+inconsistency+in+dynamic+application+log+levels"&gt;KIP-817: Fix inconsistency in dynamic application log levels&lt;/a&gt;: There are two methods for changing Kafka's log at runtime: Java Management Extensions (JMX) and the Admin API. However, there are some inconsistencies in the levels they support. For example, it's not possible to set a logger to &lt;code&gt;OFF&lt;/code&gt; via the Admin API. This KIP aims at clearing up those inconsistencies.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Community releases for Apache Kafka&lt;/h2&gt; &lt;p&gt;This section covers a few notable open source community project releases:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://a-great-day-out-with.github.io/kafka.html"&gt;A Great Day Out With... Apache Kafka&lt;/a&gt;: This is a curated list of resources, projects, tools, and products for people getting started with Kafka. The most recent update added the latest key Kafka resources people should be aware of when working with the platform.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://github.com/Shopify/sarama/releases/tag/v1.31.0"&gt;Sarama 1.31&lt;/a&gt;: Sarama is a pure &lt;a href="https://developers.redhat.com/topics/go"&gt;Go language&lt;/a&gt; Kafka client. This new release adds support for &lt;code&gt;IncrementalAlterConfigs&lt;/code&gt; in the Admin client, improves request pipelining for the producer, and as usual fixes a few bugs.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Kafka blogs and articles&lt;/h2&gt; &lt;p&gt;Here are a few of the most noteworthy Kafka-related blogs and articles published in January 2022:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://www.confluent.io/blog/5-common-pitfalls-when-using-apache-kafka/"&gt;5 common pitfalls when using Apache Kafka&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://medium.com/airwallex-engineering/kafka-streams-iterative-development-and-blue-green-deployment-fae88b26e75e"&gt;Kafka Streams: Iterative development and blue-green deployment&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://dev.to/hpgrahsl/towards-a-mongodb-backed-apache-kafka-streams-state-store-20ik"&gt;Towards a MongoDB-backed Apache Kafka Streams state store&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To learn more about Kafka, visit &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Red Hat Developer's Apache Kafka topic page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/02/10/kafka-monthly-digest-january-2022" title="Kafka Monthly Digest: January 2022"&gt;Kafka Monthly Digest: January 2022&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Mickael Maison</dc:creator><dc:date>2022-02-10T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - February 10th 2022</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2022-02-10.html" /><category term="quarkus" /><category term="java" /><category term="resteasy" /><category term="camel" /><category term="event-driven" /><category term="mta" /><category term="keycloak" /><category term="wildfly" /><category term="vertx" /><category term="kogito" /><category term="idaas" /><author><name>Pedro Silva</name><uri>https://www.jboss.org/people/pedro-silva</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2022-02-10.html</id><updated>2022-02-10T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, java, resteasy, camel, event-driven, mta, keycloak, wildfly, vertx, kogito, idaas"&gt; &lt;h1&gt;This Week in JBoss - February 10th 2022&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Hello! Welcome to the third JBoss weekly editorial of 2022. Enjoy our pick of the latest news and interesting reads from around the JBoss community.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_releases_releases_releases"&gt;Releases, releases, releases!&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Here are the releases from the JBoss Community for this edition:&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-7-1-final-released/"&gt;Quarkus 2.7.1.Final&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/01/kogito-1-16-0-released.html"&gt;Kogito 1.16.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_articles_books_and_tutorials"&gt;Articles, Books, and Tutorials&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Check out some of the recent articles about Java, Kubernetes, Quarkus, Keycloak and more…​&lt;/p&gt; &lt;div class="sect2"&gt; &lt;h3 id="_deploy_your_kie_sandbox_to_openshift"&gt;Deploy your KIE Sandbox to Openshift&lt;/h3&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/01/deploy-your-kie-sandbox-to-openshift.html"&gt;Deploy your KIE Sandbox to Openshift&lt;/a&gt; by Guilherme Caponetto&lt;/p&gt; &lt;p&gt;The Kogito Tooling release 0.16.0 includes three container images to make it easy to deploy the KIE Sandbox to an OpenShift instance.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_kie_sandbox_top_7_key_new_features"&gt;KIE Sandbox: Top 7 key new features&lt;/h3&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/02/kie-sandbox-top-7-key-new-features.html"&gt;KIE Sandbox: Top 7 key new features&lt;/a&gt; by Eder Ignatowicz&lt;/p&gt; &lt;p&gt;In the last months of 2021, the “.NEW environment” (bpmn.new, dmn.new) received a massive update, and now it’s named KIE Sandbox! Dealing with complex models and collaborating with others has just become much easier. In this blog post, let’s go for a walkthrough of the top new features of KIE Sandbox.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_artemis_monitoring_in_openshift"&gt;Artemis monitoring in OpenShift&lt;/h3&gt; &lt;p&gt;&lt;a href="https://blog.ramon-gordillo.dev/2022/02/artemis-monitoring-in-openshift/"&gt;Artemis monitoring in OpenShift&lt;/a&gt; by Ramón Gordillo&lt;/p&gt; &lt;p&gt;It is really simple to monitor the brokers deployed on OpenShift and show the metrics in grafana or configuring alerts based on the metrics. We are using the artemis version included in AMQ 7.9 deployed with the operator.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_deprecation_of_keycloak_adapters"&gt;Deprecation of Keycloak adapters&lt;/h3&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2022/02/adapter-deprecation"&gt;Deprecation of Keycloak adapters&lt;/a&gt; by Stian Thorgersen&lt;/p&gt; &lt;p&gt;This blog is a notice that the Keycloak project will be deprecating the Keycloak Adapters.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_running_arquillian_tests_from_eclipse"&gt;Running Arquillian Tests from Eclipse&lt;/h3&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/jboss-frameworks/arquillian/running-arquillian-tests-from-eclipse/"&gt;Running Arquillian Tests from Eclipse&lt;/a&gt; by F. Marchioni&lt;/p&gt; &lt;p&gt;The blog shows you how to configure the Arquillian Tests on Eclipse.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_reverse_engineer_your_jboss_as_wildfly_configuration_to_cli"&gt;Reverse engineer your JBoss AS-WildFly configuration to CLI&lt;/h3&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/jbossas/jboss-script/reverse-engineer-your-jboss-as-wildfly-configuration-to-cli/"&gt;Reverse engineer your JBoss AS-WildFly configuration to CLI&lt;/a&gt; by F. Marchioni&lt;/p&gt; &lt;p&gt;Exporting your WildFly/JBoss EAP configuration to a CLI script is something you are going to need one day or another. No worries, the project Profile Cloner comes to the rescue!&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_intelligent_data_as_a_service_idaas_example_data_insights"&gt;Intelligent data as a service (iDaaS) - Example data insights&lt;/h3&gt; &lt;p&gt;&lt;a href="https://www.schabell.org/2022/01/idaas-example-data-insights.html"&gt;Intelligent data as a service (iDaaS) - Example data insights&lt;/a&gt; by Eric D. Schabell&lt;/p&gt; &lt;p&gt;Part 5 - Data insights through the iDaaS insights architecture provides a different look at the architecture and gives us insights into how our healthcare solutions are performing.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_whats_new_for_developers_in_java_18"&gt;What’s new for developers in Java 18?&lt;/h3&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/27/whats-new-developers-java-18"&gt;What’s new for developers in Java 18?&lt;/a&gt; by Shaaf, Syed&lt;/p&gt; &lt;p&gt;This article highlights some of the features that developers can look for in the upcoming Java 18 release, including the new simple web server module, a more sophisticated way to annotate your Javadocs, and the &lt;code&gt;–finalization=disabled option&lt;/code&gt;, which lets you test how a Java application will behave when finalization is removed in a future release. See the end of the article for where to download Java 18 in early access builds.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_build_a_bootable_jar_for_cloud_ready_microservices"&gt;Build a bootable JAR for cloud-ready microservices&lt;/h3&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/26/build-bootable-jar-cloud-ready-microservices"&gt;Build a bootable JAR for cloud-ready microservices&lt;/a&gt; by Mauro Vocale&lt;/p&gt; &lt;p&gt;This article describes how to create a bootable JAR using Red Hat JBoss Enterprise Application Platform (JBoss EAP) and Jakarta EE and incorporate useful extensions, particularly a PostgreSQL database and MicroProfile capabilities.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_connect_to_an_external_postgresql_database_using_ssltls_for_red_hats_single_sign_on_technology"&gt;Connect to an external PostgreSQL database using SSL/TLS for Red Hat’s single sign-on technology&lt;/h3&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/08/31/connect-external-postgresql-database-using-ssltls-red-hats-single-sign"&gt;Connect to an external PostgreSQL database using SSL/TLS for Red Hat’s single sign-on technology&lt;/a&gt; by Olivier Rivat&lt;/p&gt; &lt;p&gt;This article shows you how to connect securely to applications and data sources using Red Hat’s single sign-on technology.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_videos"&gt;Videos&lt;/h3&gt; &lt;p&gt;Here’s my pick of this week’s YouTube videos:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=oeDFWjV43zk"&gt;KIE Drop - Custom Forms&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=7M0Tvlx-GTA"&gt;Quarkus Insights #78: Quarkus Example App Demo&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=ypqllaWE0DA"&gt;Migrate Spring Boot to Quarkus in 5 minutes&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=VeKVUNRyH6k"&gt;Workshop: Red Hat OpenShift Streams for Apache Kafka with Service Registry (Feb 2, 2022)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_books"&gt;Books&lt;/h3&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/e-books/kubernetes-native-microservices-quarkus-and-microprofile"&gt;Kubernetes Native Microservices with Quarkus and MicroProfile&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;p&gt;&lt;em&gt;That’s all for today! Please join us again in two weeks for another round of our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/pedro-silva.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Pedro Silva&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Pedro Silva</dc:creator></entry><entry><title>Create a data stream with Amazon Kinesis</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/02/09/create-data-stream-amazon-kinesis" /><author><name>Bob Reselman</name></author><id>41d5024e-7009-4875-b9f0-99ef602d7ecd</id><updated>2022-02-09T07:00:00Z</updated><published>2022-02-09T07:00:00Z</published><summary type="html">&lt;p&gt;Streaming applications process data such as video, audio, and text as a continuous flow of messages. Working with streams adds a new dimension to application programming. The difference between handling &lt;a href="https://developers.redhat.com/topics/event-driven/"&gt;events &lt;/a&gt;and data streaming is like going from drinking water one glass at a time to taking it in from a garden hose. This article shows how to get a stream up and running under &lt;a href="https://aws.amazon.com/kinesis/"&gt;Amazon Kinesis&lt;/a&gt;, a stream management service offered by Amazon Web Services (AWS).&lt;/p&gt; &lt;p&gt;Data streaming is a different way of doing business. Making it all work requires both an understanding of some basic streaming patterns and an awareness of the tools and techniques you can use to get the job done. This article takes you from the initial configuration through running a producer and checking reports on its behavior.&lt;/p&gt; &lt;h2&gt;Understanding streaming patterns&lt;/h2&gt; &lt;p&gt;As the name implies, a stream is a continuous flow of data transmitted at high-speed rates between a source and target. Probably the best example of streaming data is video content from a producer such as C-SPAN. The C-SPAN studio streams bytes of video data that make up a telecast through a streaming manager to a data center on the backend. Then that stream is forwarded onto users' computers or smart TVs. In this scenario, you can think of the video source as the producer and the viewers at home as consumers (Figure 1). Multiple consumers are easy to support, which is great for broadcasting.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/single.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/single.png?itok=U-L3jRNX" width="601" height="191" alt="A data stream goes from a producer through a streaming manager to consumers." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. A data stream goes from a producer through a streaming manager to consumers. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;There are also streaming scenarios in which several producers stream data into a streaming manager, which sends them on to consumers. This pattern is shown in Figure 2.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/multiple.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/multiple.png?itok=lkwaEuWw" width="591" height="211" alt="Data streams from multiple producers can be combined in the streaming manager and sent to consumers." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. Data streams from multiple producers can be combined in the streaming manager and sent to consumers. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;An example of this many-to-many scenario is where many browsers act as producers on the client side of a web application. Each browser sends a continuous stream of messages back to a stream manager on the server side. Each message might describe a particular mouse event from the user's browser.&lt;/p&gt; &lt;p&gt;Then, on the server side, there might be any number of consumers interested in a user's mouse events. The marketing department might be interested in the ads the user clicks on. The user experience team might want to know how much time a user takes moving the mouse around a web page before executing an action. All that any party needs to do to consume information from the stream of interest is establish a connection to the stream manager.&lt;/p&gt; &lt;p&gt;The important thing to understand about streaming is that it's very different from a request/response interaction that handles one message at a time, which is typical when web surfing or using an HTTP API such as REST. In contrast, streams involve working with an enormous amount of messages flowing continuously in one direction all the time.&lt;/p&gt; &lt;p&gt;A number of services support streaming. Google Cloud has its DataFlow service. The &lt;a href="https://www.redhat.com/en/resources/amq-streams-datasheet"&gt;streaming component&lt;/a&gt; of &lt;a href="https://developers.redhat.com/products/amq/overview"&gt;Red Hat AMQ&lt;/a&gt; is based on &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt; and integrates with &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. There are other services too. In this article, we're going to look at the Amazon Kinesis service.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; To learn how to implement message streams using OpenShift and Kafka, try this activity in the no-cost Developer Sandbox for Red Hat OpenShift: &lt;a href="https://developers.redhat.com/developer-sandbox/activities/connecting-to-your-managed-kafka-instance"&gt;Connecting to your Managed Kafka instance from the Developer Sandbox for Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Amazon Kinesis Data Streams&lt;/h2&gt; &lt;p&gt;Amazon Kinesis is made up of a number of subservices such as &lt;a href="https://aws.amazon.com/kinesis/data-streams/"&gt;Amazon Kinesis Data Streams&lt;/a&gt;, &lt;a href="https://aws.amazon.com/kinesis/data-firehose/"&gt;Amazon Kinesis Data Firehose&lt;/a&gt;, and &lt;a href="https://aws.amazon.com/kinesis/video-streams/?amazon-kinesis-video-streams-resources-blog.sort-by=item.additionalFields.createdDate&amp;amazon-kinesis-video-streams-resources-blog.sort-order=desc"&gt;Amazon Kinesis Video Streams&lt;/a&gt;. In this article, we will use Kinesis Data Streams, which is a general streaming manager.&lt;/p&gt; &lt;p&gt;Working with a Kinesis data stream is a three-step process, discussed in the sections that follow:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Create the stream.&lt;/li&gt; &lt;li&gt;Attach a user or user group to the stream. The user or group must have AWS permissions to use the stream.&lt;/li&gt; &lt;li&gt;Submit stream data as that user (group) to the stream on the backend.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;There is actually a fourth step: In order for stream data to be useful, it needs to be processed by a consumer. However, creating and using a consumer is beyond the scope of this article. In this article, we'll just get data into Kinesis Data Streams.&lt;/p&gt; &lt;h2&gt;Set up the stream on Amazon Kinesis&lt;/h2&gt; &lt;p&gt;There are a few ways to set up a Kinesis stream:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Create the stream directly using the AWS command line.&lt;/li&gt; &lt;li&gt;Run an AWS CloudFormation script.&lt;/li&gt; &lt;li&gt;Use the AWS dashboard.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In this article, we'll use the dashboard.&lt;/p&gt; &lt;p&gt;Figure 3 illustrates the process for creating the Kinesis stream.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/stream.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/stream.png?itok=btRawS8b" width="1440" height="841" alt="From the AWS Services page, choose Kinesis Data Streams and create a stream." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. From the AWS Services page, choose Kinesis Data Streams and create a stream. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The steps are as follows:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Search the AWS &lt;strong&gt;Services&lt;/strong&gt; main page for the term "Kinesis "and then select the &lt;strong&gt;Kinesis&lt;/strong&gt; service.&lt;/li&gt; &lt;li&gt;Select the &lt;strong&gt;Kinesis Data Streams&lt;/strong&gt; option in the dialog that appears.&lt;/li&gt; &lt;li&gt;Enter the name of the new data stream in the &lt;strong&gt;Data stream name&lt;/strong&gt; box. In this case, we'll name the stream &lt;code&gt;general_stream&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create data stream&lt;/strong&gt; button at the bottom of the &lt;strong&gt;Create data stream&lt;/strong&gt; page.&lt;/li&gt; &lt;li&gt;The newly created stream is displayed on the stream's page, in this case, &lt;strong&gt;Amazon Kinesis→Data streams→&lt;/strong&gt;&lt;strong&gt;general_stream&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Create a user for the stream&lt;/h2&gt; &lt;p&gt;After the stream is created, you'll need to create a user in AWS that has to write access to the stream. There are two ways to grant permission. You can create an AWS group that has write access permission to the stream and then add an existing or new user to the group. Another way is to create a new user and give that user the required permission to write to the stream.&lt;/p&gt; &lt;p&gt;For this article, we'll create a new user specifically dedicated to writing to the stream. Please be advised that we're doing this for demonstration purposes only. At the enterprise level, granting permissions to AWS resources is a very formal process that can vary from company to company. Some companies create user groups with specific permissions and assign users to that group. Other companies assign permissions on a user-by-user basis. In this case, we'll take the single-user approach.&lt;/p&gt; &lt;p&gt;Figure 4 illustrates the process.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/user_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/user_1.png?itok=LHR9v4ka" width="1440" height="675" alt="Create a user and save the access information for later use." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4. Create a user and save the access information for later use. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The steps are as follows:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Select the &lt;strong&gt;Identity and Access Management (IAM)&lt;/strong&gt; service from the AWS &lt;strong&gt;Services&lt;/strong&gt; page. In the IAM page, select &lt;strong&gt;Users&lt;/strong&gt;. The Users page will appear.&lt;/li&gt; &lt;li&gt;Click the button labeled &lt;strong&gt;Add users&lt;/strong&gt; on the upper right of the page. The &lt;strong&gt;Add user&lt;/strong&gt; page appears.&lt;/li&gt; &lt;li&gt;Enter the name of the user. In this case, we'll enter the name &lt;code&gt;my_kinesis_user&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Check the &lt;strong&gt;Access key&lt;/strong&gt; checkbox. An access key ID and secret access key are generated for this user. These two credentials are very important. You'll use them to allow write access to the stream from outside of AWS.&lt;/li&gt; &lt;li&gt;Once you fill out the first page in the create user process, you'll be presented with pages for setting up permissions, tags, etc. You can just click through these pages without making any entries. We'll set up permissions later on.&lt;/li&gt; &lt;li&gt;Finally, you have created the user. The &lt;strong&gt;Success&lt;/strong&gt; page displays the access key ID and the secret access key. Also, there's a button labeled &lt;strong&gt;Download .csv&lt;/strong&gt;. Click this button to download the &lt;code&gt;.csv&lt;/code&gt; file that contains the access key ID and the secret access key information to your local machine. You'll need that information when creating programs that write to a Kinesis stream as the user you've just created.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Define access to the Amazon Kinesis stream&lt;/h2&gt; &lt;p&gt;Now that you've created the special user, you need to create a policy granting write permission to the stream. After the policy is created, you'll assign it to the user that was created in the previous section.&lt;/p&gt; &lt;p&gt;There are a lot of permissions to choose from. However, if you know exactly the permissions you want to assign to the policy, you can define them in JSON format and apply them directly. This is the approach we'll take. Our policy is defined in the following JSON, which describes all the permissions a user needs to have write access to any Kinesis stream:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;{ "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Action": [ "kinesis:SubscribeToShard", "kinesis:ListShards", "kinesis:PutRecords", "kinesis:GetShardIterator", "kinesis:DescribeStream", "kinesis:DescribeStreamSummary", "kinesis:DescribeStreamConsumer", "kinesis:RegisterStreamConsumer", "kinesis:PutRecord" ], "Resource": "*" } ] }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Figure 5 illustrates how to add the permissions.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/policy.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/policy.png?itok=TwXF53R8" width="1440" height="608" alt="Create a policy by pasting in permissions in JSON format and giving the policy a name." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5. Create a policy by pasting in permissions in JSON format and giving the policy a name. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The steps are as follows:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;In the &lt;strong&gt;Identity and Access Management (IAM)&lt;/strong&gt; page, click &lt;strong&gt;Policies&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create Policy&lt;/strong&gt; button, shown on the right side of the screen. This displays the permissions dialog.&lt;/li&gt; &lt;li&gt;Select the &lt;strong&gt;JSON&lt;/strong&gt; tab in the permissions dialog. Substitute the JSON shown earlier for the placeholder text that appeared initially. Click through the next set of pages until you get to the &lt;strong&gt;Review policy&lt;/strong&gt; page.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Review policy&lt;/strong&gt; page, enter a name for the policy in the text box. In this case, we use the name &lt;code&gt;general-write-access-to-kinesis&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create policy&lt;/strong&gt; button.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Now that we've created the policy, we need to apply it to the previously created user named &lt;code&gt;my_kinesis_user&lt;/code&gt;. That task is the subject of the next section.&lt;/p&gt; &lt;h2&gt;Enable access to the Amazon Kinesis stream&lt;/h2&gt; &lt;p&gt;At this point we have a special user named &lt;code&gt;my_kinesis_user&lt;/code&gt; and a policy named &lt;code&gt;general-write-access-to-kinesis&lt;/code&gt;. Now we need to attach the policy to the user. Figure 6 illustrates the process.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/attach.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/attach.png?itok=gkPspfEY" width="1440" height="1103" alt="Attach your policy to your user on the Users page of the IAM site." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6. Attach your policy to your user on the Users page of the IAM site. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The steps are as follows:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;In the &lt;strong&gt;Identity and Access Management (IAM)&lt;/strong&gt; page, click &lt;strong&gt;Users&lt;/strong&gt; on the left side to return to the &lt;strong&gt;Users&lt;/strong&gt; page. A list of users will appear. Click on the user named &lt;code&gt;my_kinesis_user&lt;/code&gt;. A summary page for that user will appear.&lt;/li&gt; &lt;li&gt;Under the &lt;strong&gt;Permissions&lt;/strong&gt; tab, click the &lt;strong&gt;Add permissions&lt;/strong&gt; button. The &lt;strong&gt;Add permissions&lt;/strong&gt; page will appear.&lt;/li&gt; &lt;li&gt;Select the button labeled &lt;strong&gt;Attach existing policies directly&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Filter policies&lt;/strong&gt; text box, enter the term, &lt;code&gt;general&lt;/code&gt;. As you type the letters, the custom policy &lt;code&gt;general-write-access-to-kinesis&lt;/code&gt; appears. This is the policy you created previously.&lt;/li&gt; &lt;li&gt;Select the checkbox associated with the policy.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Next: Review&lt;/strong&gt; button on the right side.&lt;/li&gt; &lt;li&gt;Review the policy assignment and then click the &lt;strong&gt;Add permissions&lt;/strong&gt; button.&lt;/li&gt; &lt;li&gt;The summary page for the user shows its Amazon Resource Name (ARN).&lt;/li&gt; &lt;li&gt;On the user's &lt;strong&gt;Permissions&lt;/strong&gt; tab, the &lt;code&gt;general-write-access-to-kinesis&lt;/code&gt; policy is shown applying to the user.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;At this point, you can create an application that uses the AWS SDK to write to a Kinesis stream as the user &lt;code&gt;my_kinesis_user&lt;/code&gt;. The program will need the access key ID and secret access key ID credentials you created earlier.&lt;/p&gt; &lt;h2&gt;Stream data to Amazon Kinesis using the AWS SDK&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; application we'll use to demonstrate the use of the AWS SDK is stored as a GitHub project named &lt;code&gt;kinesis-streamer&lt;/code&gt;. The application is intended to show you how to write to a Kinesis stream from outside of AWS. You can find the source code in &lt;a href="https://github.com/reselbob/kinesis-streamer"&gt;my GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Figure 7 shows a screenshot of three instances of &lt;code&gt;kinesis-streamer&lt;/code&gt; submitting data to a single Kinesis stream. This is a real-world example of the "many producer to single stream" pattern shown in Figure 2.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/streamer.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/streamer.png?itok=tNTwoPZz" width="1139" height="670" alt="Three produces are streaming JSON into Kinesis." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7. Three produces are streaming JSOn into Kinesis. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Each instance of &lt;code&gt;kinesis-streamer&lt;/code&gt; creates a number of &lt;code&gt;cron&lt;/code&gt; jobs that fire every second. Each &lt;code&gt;cron&lt;/code&gt; job sends a number of messages to a particular Kinesis stream. By default, the application creates ten &lt;code&gt;cron&lt;/code&gt; jobs, each of which submits ten messages to the Kinesis stream.&lt;/p&gt; &lt;p&gt;A programmer binds to an Amazon Kinesis stream by setting values to environment variables that allow the application to write to the defined Kinesis stream as a particular AWS user. The user is identified by its AWS access key ID and secret access key. Also, the target Kinesis stream is declared by an environment variable. In addition, the developer can set environment variables to override the default number of &lt;code&gt;cron&lt;/code&gt; jobs and messages to create. You can read the details of how to get the application up and running at its &lt;a href="https://github.com/reselbob/kinesis-streamer/blob/main/README.md"&gt;GitHub repository&lt;/a&gt;. The following settings illustrate how to configure the environment variables:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;AWS_ACCESS_KEY_ID="a^ACcEsS_KEY_tokEN!" AWS_SECRET_ACCESS_KEY="a^SecRET_accESS_k3y_TOken" AWS_KINESIS_STREAM_NAME=my-stream-kinesis CRON_JOBS_TO_GENERATE=50 MESSAGES_PER_CRON_JOB=20&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; Be advised that the values assigned to the environment variables &lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt; and &lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt; shown in the preceding snippet are only fictitious placeholder values.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;kinesis-streamer&lt;/code&gt; project uses the &lt;a href="https://aws.amazon.com/sdk-for-javascript/"&gt;AWS SDK for JavaScript&lt;/a&gt; to create a JavaScript client running under Node.js that writes to a Kinesis stream. There are many other ways to send data to Kinesis from both outside and inside AWS. You can use a technology such as AWS's &lt;a href="https://awslabs.github.io/amazon-kinesis-data-generator/web/help.html"&gt;Kinesis Data Generator (KDG)&lt;/a&gt; to send messages externally to an internal Kinesis stream. Or you can use an AWS Lambda function to send messages internally to Kinesis from within AWS. Of course, you can always create your own program using one of the many programming languages that are supported by the AWS SDK.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;kinesis-streamer&lt;/code&gt; project demonstrates such possibilities. So you might want to examine the &lt;a href="https://github.com/reselbob/kinesis-streamer"&gt;source code in the demonstration project&lt;/a&gt; to learn the details of working with Kinesis streams using the AWS SDK. The code is well documented to make it easier for developers to absorb the details.&lt;/p&gt; &lt;h2&gt;Get performance data using Kinesis stream reports&lt;/h2&gt; &lt;p&gt;Message activity in a Kinesis stream can be monitored using a number of graphical reports that AWS provides out of the box. From a developer's point of view, at the very least, you'll want to know whether the messages your application emits are actually making it into the Kinesis stream manager. These reports will tell you that information at a glance.&lt;/p&gt; &lt;p&gt;For example, Figure 8 shows reports for incoming data. These reports indicate that the stream received records. As simple as it sounds, this is very useful information.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/reporting.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/reporting.png?itok=sX83EaNY" width="931" height="510" alt="The Kinesis report for incoming data shows several statistics, such as the amount of messages and bytes received." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8. The Kinesis report for incoming data shows several statistics, such as the amount of messages and bytes received. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;There are a number of other reports available for monitoring stream activity, such as &lt;strong&gt;latency&lt;/strong&gt;, &lt;strong&gt;errors&lt;/strong&gt;, and &lt;strong&gt;throughput reached&lt;/strong&gt;, to name a few&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Kinesis reporting allows developers to see the results of their application's streaming activity. This information is useful not only for smoke testing on the fly, but also for more extensive troubleshooting when trying to optimize the overall performance of a Kinesis stream.&lt;/p&gt; &lt;h2&gt;Next steps&lt;/h2&gt; &lt;p&gt;The information in this article shows you how to create a Kinesis data stream and how to send data to that stream. However, as mentioned earlier, simply sending data to a stream isn't enough. In order for the stream to be useful, there need to be consumers on the other end of the stream that are processing the incoming messages.&lt;/p&gt; &lt;p&gt;Writing consumers is a topic worthy of an article all to itself. Each consumer satisfies a specific use case. Not only do developers need to create programming logic for their producers and consumers, but they also need to decide which programming language best suits the need at hand. For example, Node.js JavaScript applications are operating system agnostic and a bit simpler to program in terms of language syntax, yet consumers written in Go run a lot faster. There's a lot to consider.&lt;/p&gt; &lt;p&gt;As mentioned at the beginning of this article, data streams add a new dimension to application development. Applications that use data streams make online services such as Netflix and Hulu possible. Yet technology does not stand still. New use cases are sure to emerge, and new streaming technologies will appear to meet many of the new challenges that older technologies can't address. As a result, developers who master the intricacies of working with data streams today are sure to enjoy a prosperous career in the future that's to come.&lt;/p&gt; &lt;h3&gt;Resources&lt;/h3&gt; &lt;p&gt;Check out the following resources to learn more:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/reselbob/kinesis-streamer"&gt;Demo source code&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/developer-sandbox/activities/connecting-to-your-managed-kafka-instance"&gt;Activity: Connecting to your Managed Kafka instance from the Developer Sandbox for Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://aws.amazon.com/kinesis/data-streams/"&gt;Amazon Kinesis Data Streams&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/02/09/create-data-stream-amazon-kinesis" title="Create a data stream with Amazon Kinesis"&gt;Create a data stream with Amazon Kinesis&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bob Reselman</dc:creator><dc:date>2022-02-09T07:00:00Z</dc:date></entry><entry><title>Automate and deploy a JBoss EAP cluster with Ansible</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/02/08/automate-and-deploy-jboss-eap-cluster-ansible" /><author><name>Romain Pelisse</name></author><id>edefe4d3-3bd3-407d-aaae-110b95ce03f3</id><updated>2022-02-08T07:00:00Z</updated><published>2022-02-08T07:00:00Z</published><summary type="html">&lt;p&gt;In my series introducing &lt;a href="https://developers.redhat.com/blog/2020/11/06/wildfly-server-configuration-with-ansible-collection-for-jcliff-part-1/"&gt;WildFly server configuration with Ansible collection for JCliff&lt;/a&gt;, I described how developers can use &lt;a href="https://www.ansible.com"&gt;Ansible&lt;/a&gt; to manage a standalone &lt;a href="https://developers.redhat.com/eap/download"&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt; (JBoss EAP) instance. I've also written about using Ansible to &lt;a href="https://developers.redhat.com/articles/2021/08/30/automate-red-hat-jboss-web-server-deployments-ansible"&gt;automate Apache Tomcat and Red Hat JBoss Web Server deployments&lt;/a&gt;. In this article, we'll go a bit deeper and use Ansible to deploy a fully operational &lt;a href="http://www.mastertheboss.com/jbossas/jboss-cluster/clustering-wildfly-application-server/"&gt;cluster of JBoss EAP instances&lt;/a&gt;. I'll show you how to automate the setup of each JBoss EAP instance and how to configure the network requirements—notably, fault tolerance and high availability—using features provided by the &lt;a href="https://www.wildfly.org"&gt;WildFly&lt;/a&gt; Ansible collection.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; This article assumes that you have prior knowledge of both Ansible and basic JBoss EAP/WildFly installation. Visit the &lt;a href="https://developers.redhat.com/courses/ansible/getting-started"&gt;Ansible courses page&lt;/a&gt; to learn the fundamentals of using Ansible.&lt;/p&gt; &lt;h2&gt;Use case: Deploying a JBoss EAP cluster with Ansible&lt;/h2&gt; &lt;p&gt;For this demonstration, we want to set up and run three JBoss EAP instances in a cluster. In this context, the application servers must communicate with each other to synchronize the content of the application's session. This configuration guarantees that, if one instance fails while processing a request, another one can pick up the work without any data loss.&lt;/p&gt; &lt;p&gt;We'll use a &lt;a href="https://en.wikipedia.org/wiki/Multicast"&gt;multicast&lt;/a&gt; to discover the members of the cluster and ensure that the cluster's formation is fully automated and dynamic.&lt;/p&gt; &lt;h2&gt;Step 1: Install Ansible collection for WildFly&lt;/h2&gt; &lt;p&gt;To follow this example, you need to install the Ansible collection that provides support for JBoss EAP. Named after JBoss EAP's upstream project, &lt;a href="http://github.com/ansible-middleware/wildfly/"&gt;Ansible collection for WildFly&lt;/a&gt; is part of the &lt;code&gt;middleware_automation&lt;/code&gt; collections and supplies a set of roles to simplify automation:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-galaxy collection install middleware_automation.wildfly Process install dependency map Starting collection install process Installing 'middleware_automation.wildfly:0.0.2' to '/root/.ansible/collections/ansible_collections/middleware_automation/wildfly' Installing 'middleware_automation.redhat_csp_download:1.2.1' to '/root/.ansible/collections/ansible_collections/middleware_automation/redhat_csp_download&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note the following:&lt;/p&gt; &lt;p&gt;The content of Ansible collection for WildFly comes from the &lt;a href="https://galaxy.ansible.com/wildfly/jcliff"&gt;Ansible collection for JCliff&lt;/a&gt;, which is used to help deploy applications and fine-tune server configurations. To keep things simple, we won't use JCliff's features in this example. The part of the WildFly collection that we are using has been separated from the JCliff collection so that developers can use the WildFly features without having to install JCliff. Features of JCliff are not required for all use cases involving JBoss EAP.&lt;/p&gt; &lt;p&gt;Additionally, note that the &lt;code&gt;middleware_automation&lt;/code&gt; collections are provided through &lt;a href="https://galaxy.ansible.com"&gt;Ansible Galaxy&lt;/a&gt; and are not certified for &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt;. Those certified collections are provided by &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_ansible_automation_platform/1.2/html/getting_started_with_red_hat_ansible_automation_hub/index"&gt;Red Hat Ansible Automation Hub&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Step 2: Set up the JBoss EAP cluster&lt;/h2&gt; &lt;p&gt;A typical JBoss EAP cluster has several machines, each operating a dedicated instance. In this case, for the simplicity of testing and reproducibility on a development system, we are going to use just one machine running several instances of JBoss EAP. The WildFly collection for Ansible makes it relatively easy to set up this architecture and provides all the required plumbing.&lt;/p&gt; &lt;p&gt;There are two parts to setting up the JBoss EAP cluster:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;&lt;strong&gt;Install JBoss EAP on the hosts.&lt;/strong&gt; This installation involves authenticating against the Red Hat Network (RHN), downloading the archive, and decompressing the archive in the appropriate directory (&lt;code&gt;JBOSS_HOME&lt;/code&gt;). These tasks are handled by the &lt;code&gt;wildfly_install&lt;/code&gt; role supplied by &lt;code&gt;wildfly&lt;/code&gt; collection.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Create the configuration files to run several instances of JBoss EAP.&lt;/strong&gt; Because we're running multiple instances on a single host, you also need to ensure that each instance has its own subdirectories and set of ports, so that the instances can coexist and communicate. Fortunately, this functionality is provided by a role within the Ansible collection called &lt;code&gt;wildfly_systemd&lt;/code&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Note that if the variables &lt;code&gt;rhn_username&lt;/code&gt; and &lt;code&gt;rhn_password&lt;/code&gt; are defined, the collection automatically downloads the latest available version of JBoss EAP. If not, the &lt;code&gt;wildfly_install&lt;/code&gt; role fetches the latest upstream WildFly version. To avoid adding the credentials in our playbook, we incorporate them into a separate file named &lt;code&gt;rhn-creds.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- rhn_username: rhn_password: jboss_eap_rhn_id: &lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; We could use &lt;a href="https://docs.ansible.com/ansible/latest/user_guide/vault.html"&gt;Ansible's vault&lt;/a&gt; feature to safely encrypt the credential values, but doing that is out of the scope of this article.&lt;/p&gt; &lt;h3&gt;Ansible playbook to install JBoss EAP&lt;/h3&gt; &lt;p&gt;Here is our Ansible playbook for installing and configuring JBoss EAP:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - name: "JBoss EAP installation and configuration" hosts: "{{ hosts_group_name | default('localhost') }}" become: yes vars: wildfly_install_workdir: '/opt' wildfly_version: '7.4' install_name: jboss-eap wildfly_archive_filename: "{{ install_name }}-{{ wildfly_version }}.zip" wildfly_user: "{{ install_name }}" wildfly_config_base: standalone-ha.xml wildfly_home: "{{ wildfly_install_workdir }}/{{ install_name }}-{{ wildfly_version }}" jboss_eap_rhn_id: 99481 instance_http_ports: - 8180 - 8280 - 8380 app: name: 'info-1.1.war' url: 'https://drive.google.com/uc?export=download&amp;id=1w9ss5okctnjUvRAxhPEPyC7DmbUwmbhb' collections: - middleware_automation.redhat_csp_download - middleware_automation.wildfly roles: - redhat_csp_download - wildfly_install tasks: - name: "Set up for WildFly instance {{ item }}" include_role: name: wildfly_systemd vars: wildfly_config_base: 'standalone-ha.xml' wildfly_basedir_prefix: "/opt/{{ inventory_hostname }}" wildfly_config_name: "{{ install_name }}" wildfly_port_range_offset: -1 wildfly_instance_name: "{{ install_name }}" instance_id: "{{ item }}" service_systemd_env_file: "/etc/eap-{{ item }}.conf" service_systemd_conf_file: "/usr/lib/systemd/system/jboss-eap-{{ item }}.service" loop: "{{ range(0,3) | list }}" post_tasks: - set_fact: instance_http_ports: - 8180 - 8280 - 8380 - wait_for: port: "{{ item }}" loop: "{{ instance_http_ports }}" - name: "Checks that WildFly server is running and accessible" get_url: url: "http://localhost:{{ item }}/" dest: '/dev/null' loop: "{{ instance_http_ports }}" &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Run the playbook&lt;/h3&gt; &lt;p&gt;Now, let's run our Ansible playbook and check the resulting output:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# ansible-playbook -e @creds.yml jboss_eap.yml PLAY [WildFly installation and configuration] ****************************************************************** TASK [Gathering Facts] ***************************************************************************************** ok: [localhost] TASK [middleware_automation.redhat_csp_download.redhat_csp_download : assert] … TASK [Checks that WildFly server is running and accessible] **************************************************** ok: [localhost] =&gt; (item=8180) ok: [localhost] =&gt; (item=8280) ok: [localhost] =&gt; (item=8380) PLAY RECAP ***************************************************************************************************** localhost : ok=90 changed=0 unreachable=0 failed=0 skipped=14 rescued=0 ignored=0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Although the playbook is quite short, it performs almost 100 tasks. First, it automatically installs the dependencies for &lt;code&gt;middleware_automation.redhat_csp_download&lt;/code&gt; by adding the associated role. Then, the &lt;code&gt;wildfly_install&lt;/code&gt; role uses the provided credentials to connect to RHN and download the &lt;code&gt;jboss-eap-7.4.zip&lt;/code&gt; file. Finally, once those steps have been completed successfully, the &lt;code&gt;wildfly_systemd&lt;/code&gt; role sets up three distinct services, each with its own set of ports and directory layout to store instance-specific data.&lt;/p&gt; &lt;p&gt;Note that the JBoss EAP installation is &lt;strong&gt;not duplicated&lt;/strong&gt;. All of the binaries live under the &lt;code&gt;/opt/jboss-eap-74&lt;/code&gt; directory. The separate directories simply store the runtime data for each instance.&lt;/p&gt; &lt;p&gt;On top of everything, we configured the instances to use the &lt;code&gt;standalone-ha.xml&lt;/code&gt; configuration as the baseline, so they are already set up for clustering.&lt;/p&gt; &lt;h2&gt;Step 3: Confirm the JBoss EAP instance and services are running&lt;/h2&gt; &lt;p&gt;The playbook confirms that each instance can be reached through its own HTTP port. We can also verify that the services are running by using the &lt;code&gt;systemctl&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# systemctl status jboss-eap-* ● jboss-eap-0.service - JBoss EAP (standalone mode) Loaded: loaded (/usr/lib/systemd/system/jboss-eap-0.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2021-12-23 12:31:41 UTC; 1min 55s ago Main PID: 1138 (standalone.sh) Tasks: 70 (limit: 1638) Memory: 532.3M CGroup: /system.slice/jboss-eap-0.service ├─1138 /bin/sh /opt/jboss-eap-7.4/bin/standalone.sh -c jboss-eap-0.xml -b 0.0.0.0 -Djboss.server.con&gt; └─1261 java -D[Standalone] -server -verbose:gc -Xloggc:/opt/localhost0/log/gc.log -XX:+PrintGCDetail&gt; Dec 23 12:31:44 7b38800644ee standalone.sh[1138]: 12:31:44,548 INFO [org.jboss.as.patching] (MSC service thread) Dec 23 12:31:44 7b38800644ee standalone.sh[1138]: 12:31:44,563 WARN [org.jboss.as.domain.management.security] &gt; Dec 23 12:31:44 7b38800644ee standalone.sh[1138]: 12:31 …&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 4: Deploy an application to the JBoss EAP cluster&lt;/h2&gt; &lt;p&gt;At this point, the three JBoss EAP instances are configured for clustering. However, no applications were deployed, so the cluster is not active (there is nothing to keep synchronized between all the instances).&lt;/p&gt; &lt;p&gt;Let's modify our Ansible playbook to deploy a simple application to all three JBoss EAP instances. To achieve this, we'll leverage another role provided by the &lt;code&gt;wildfly&lt;/code&gt; collection: &lt;code&gt;jboss_eap&lt;/code&gt;. This role includes a set of tasks generally focused on features specific to JBoss EAP.&lt;/p&gt; &lt;p&gt;In our case, we will use the &lt;code&gt;jboss_cli.yml&lt;/code&gt; task file, which encapsulates the running of JBoss command-line interface (CLI) queries:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;… - name: "Ensures webapp {{ app.name }} has been retrieved from {{ app.url }}" get_url: url: "{{ app.url }}" dest: "{{ wildfly_install_workdir }}/{{ app.name }}" - name: "Deploy webapp" include_role: name: jboss_eap tasks_from: jboss_cli.yml vars: jboss_home: "{{ wildfly_home }}" query: "'deploy --force {{ wildfly_install_workdir }}/{{ app.name }}'" jboss_cli_controller_port: "{{ item }}" loop: - 10090 - 10190 - 10290 …&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, we will once again execute our playbook so that the web application is deployed on all instances. Once the automation completes successfully, the deployment will trigger the formation of the cluster.&lt;/p&gt; &lt;h2&gt;Step 5: Verify the JBoss EAP cluster and application deployment&lt;/h2&gt; &lt;p&gt;You can verify the JBoss EAP cluster formation by looking at the log files of any of the three JBoss EAP instances:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;… 021-12-23 15:02:08,252 INFO [org.infinispan.CLUSTER] (thread-7,ejb,jboss-eap-0) ISPN000094: Received new cluster view for channel ejb: [jboss-eap-0|2] (3) [jboss-eap-0, jboss-eap-1, jboss-eap-2] …&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To be thorough, you can also check that the application is properly deployed and accessible. To validate the application's operation, we can simply add a separate Ansible playbook called &lt;code&gt;validate.yml&lt;/code&gt;. We can then import the new playbook into our &lt;code&gt;playbook.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;post_tasks: - include_tasks: validate.yml loop: "{{ instance_http_ports }}"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;validate.yml&lt;/code&gt; file contains the following:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - assert: that: - item is defined - wait_for: port: "{{ item }}" - name: "Checks that WildFly server is running and accessible on port {{ item }}" get_url: url: "http://localhost:{{ item }}/" dest: '/dev/null' changed_when: False - include_tasks: info.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You might have noticed that we include another playbook, &lt;code&gt;info.yml&lt;/code&gt;, which is here:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - assert: that: - item is defined quiet: true - set_fact: result_file: "/tmp/info-{{ item }}.txt" - get_url: url: "http://localhost:{{ item }}/info/" dest: "{{ result_file }}" changed_when: False - slurp: src: "{{ result_file }}" register: info_res - debug: msg: "{{ info_res['content'] | b64decode }}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To complete the exercise, we can run the validation playbook and see whether it confirms that our setup is fully functional:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;TASK [get_url] ******************************************************************************** changed: [localhost] TASK [slurp] ********************************************************************************** ok: [localhost] TASK [debug] ********************************************************************************** ok: [localhost] =&gt; { "msg": "Request received&lt;br/&gt;Requested URL:\t\t\thttp://localhost:8380/info/&lt;br/&gt;Runs on node:\t\t\tda953ac17443 [IP: 10.0.2.100 ]&lt;br/&gt;Requested by:\t\t\t127.0.0.1 [IP: 127.0.0.1, port: 40334 ]&lt;br/&gt;JBOSS_ID:\t\t\tnull&lt;br/&gt;" }&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Next steps with Ansible and JBoss EAP&lt;/h2&gt; &lt;p&gt;In this article, we've fully automated the setup and configuration of a three-instance cluster of JBoss EAP, along with an example web application as a workload. The playbook that we created for the automation is simple and straightforward. Most importantly, we were able to focus primarily on deploying the application. The WildFly collection for Ansible provided all the plumbing we needed to set up the JBoss EAP cluster.&lt;/p&gt; &lt;p&gt;You can find the source code for the example in the &lt;a href="https://github.com/ansible-middleware/wildfly-cluster-demo"&gt;WildFly cluster demo&lt;/a&gt; GitHub repository. For a more complex scenario, see the &lt;a href="https://github.com/ansible-middleware/flange-demo"&gt;Flange project demo&lt;/a&gt;, which adds to the JBoss EAP cluster an instance of Red Hat's &lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;single sign-on technology&lt;/a&gt;, using &lt;a href="https://developers.redhat.com/products/datagrid/overview"&gt;Red Hat Data Grid&lt;/a&gt; as a cache and &lt;a href="https://www.redhat.com/en/resources/jboss-core-services-collection-datasheet"&gt;Red Hat Middleware Core Services Collection&lt;/a&gt; as a load balancer.&lt;/p&gt; &lt;p&gt;See these resources to learn more about using Ansible with JBoss EAP:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/08/30/automate-red-hat-jboss-web-server-deployments-ansible"&gt;Automate Red Hat JBoss Web Server deployments with Ansible&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/09/28/set-modcluster-red-hat-jboss-web-server-ansible"&gt;Set up mod_cluster for Red Hat JBoss Web Server with Ansible&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2020/11/06/wildfly-server-configuration-with-ansible-collection-for-jcliff-part-1"&gt;WildFly server configuration with Ansible collection for JCliff (three-part series)&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;For more hands-on learning, in short, interactive courses, see the &lt;a href="https://developers.redhat.com/courses/ansible"&gt;Ansible courses page&lt;/a&gt;. Also, be sure to check out &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt; for provisioning, deploying, and managing IT infrastructure across cloud, virtual, and physical environments.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/02/08/automate-and-deploy-jboss-eap-cluster-ansible" title="Automate and deploy a JBoss EAP cluster with Ansible"&gt;Automate and deploy a JBoss EAP cluster with Ansible&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Romain Pelisse</dc:creator><dc:date>2022-02-08T07:00:00Z</dc:date></entry><entry><title>Quarkus 2.7.1.Final released - Maintenance release</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-7-1-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-7-1-final-released/</id><updated>2022-02-08T00:00:00Z</updated><published>2022-02-08T00:00:00Z</published><summary type="html">Today, we released Quarkus 2.7.1.Final, our first maintenance release for our 2.7 release train. It contains bugfixes and documentation improvements, but also a couple of new features, filling a few gaps. It is a safe upgrade for anyone already using 2.7. If you are not using 2.7 already, please refer...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2022-02-08T00:00:00Z</dc:date></entry><entry><title>Investigating the cost of Open vSwitch upcalls in Linux</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/02/07/investigating-cost-open-vswitch-upcalls-linux" /><author><name>Eelco Chaudron</name></author><id>bc412619-1b36-4bc8-88ac-f33efd3af3ba</id><updated>2022-02-07T07:00:00Z</updated><published>2022-02-07T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://www.openvswitch.org/"&gt;Open vSwitch (OVS)&lt;/a&gt;, which many data centers run on &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; systems for advanced networking functions, adds a certain amount of overhead for new datapath flows. This article introduces the &lt;code&gt;upcall_cost.py&lt;/code&gt; script, written in &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;, which tracks packets through the kernel's invocation of OVS and displays statistics that can help with troubleshooting performance in applications and data centers, as well as the kernel and OVS itself.&lt;/p&gt; &lt;p&gt;My interest in this question was triggered when some people argued that it takes too long for the kernel module to bring a packet to the &lt;code&gt;ovs-vswitchd&lt;/code&gt; in userspace to do the lookup. However, I have not seen anyone backing up this complaint with data. This article offers tools that can help research this question and many others. I'll describe the script and its output, then show data from two interesting scenarios: A 16-node &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; cluster and a lab benchmark.&lt;/p&gt; &lt;h2 id="measuring-the-cost"&gt;The upcall_cost.py script&lt;/h2&gt; &lt;p&gt;Up till now, there was no easy way to measure the overhead introduced by the OVS kernel module, whose tasks include sending packets to the &lt;code&gt;ovs-vswitchd&lt;/code&gt; user space daemon for the OpenFlow lookup, programming the kernel datapath flow, and re-inserting the packets for forwarding. The &lt;code&gt;upcall_cost.py&lt;/code&gt; script was introduced as part of an OVS &lt;a href="https://patchwork.ozlabs.org/project/openvswitch/list/?series=276751&amp;state=*"&gt;patch series&lt;/a&gt; and demoed as part of the 2021 OVS conference presentation titled &lt;a href="http://www.openvswitch.org/support/ovscon2021/#T1"&gt;Debugging OVS using static tracepoints&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;But just having a script to get these statistics does not make capturing and analyzing this data straightforward. Every environment and use case is different and hence might give different results. Later on, we will discuss some data based on a 16-cluster &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; setup emulating a customer environment.&lt;/p&gt; &lt;h3 id="what-will-upcall_costpy-measure"&gt;Running the upcall_cost.py script&lt;/h3&gt; &lt;p&gt;I've devoted a section of this article to explaining how to run the script, because several of the options are important.&lt;/p&gt; &lt;p&gt;Make sure that, along with downloading the script, you have installed all the required Python modules. See the script's header for the requirements.&lt;/p&gt; &lt;p&gt;The script queries the configured OVS ports only at startup. So if you have a dynamic environment, you might need to modify the script or make sure all ports are configured before starting the script.&lt;/p&gt; &lt;p&gt;Executing the script with the &lt;code&gt;--help&lt;/code&gt; option shows all the available options:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;# ./upcall_cost.py --help usage: upcall_cost.py [-h] [-b BUCKETS] [--buffer-page-count NUMBER] [-D [DEBUG]] [-f [64-2048]] [--handler-filter HANDLERS] [-P [64-2048]] [-p VSWITCHD_PID] [-q] [-r FILE] [--sets] [-s EVENTS] [-w FILE] optional arguments: -h, --help show this help message and exit -b BUCKETS, --histogram-buckets BUCKETS Number of buckets per histogram, default 20 --buffer-page-count NUMBER Number of BPF ring buffer pages, default 1024 -D [DEBUG], --debug [DEBUG] Enable eBPF debugging -f [64-2048], --flow-key-size [64-2048] Set maximum flow key size to capture, default 64 --handler-filter HANDLERS Post processing handler thread filter -P [64-2048], --packet-size [64-2048] Set maximum packet size to capture, default 256 -p VSWITCHD_PID, --pid VSWITCHD_PID ovs-vswitch's PID -q, --quiet Do not show individual events -r FILE, --read-events FILE Read events from FILE instead of installing tracepoints --sets Dump content of data sets -s EVENTS, --stop EVENTS Stop after receiving EVENTS number of trace events -w FILE, --write-events FILE Write events to FILE&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Although the defaults are fine for most options, a few are worth discussing here.&lt;/p&gt; &lt;p&gt;First, I recommend running the script with the &lt;code&gt;--write-events&lt;/code&gt; or &lt;code&gt;-w&lt;/code&gt; option to save a copy of the trace information. This output can be used later to analyze the results.&lt;/p&gt; &lt;p&gt;Initially, you might want to see all the output, but it could be voluminous and introduce a delay. Therefore, after you have verified that events are coming in, I suggest running the command with the &lt;code&gt;--quiet&lt;/code&gt; or &lt;code&gt;-q&lt;/code&gt; option.&lt;/p&gt; &lt;p&gt;To make sure you don't miss any trace events, your &lt;a href="https://www.kernel.org/doc/html/latest/bpf/ringbuf.html"&gt;BPF ring buffer&lt;/a&gt; needs to be configured to be large enough to keep the events. The following example shows that the default configured size of 1,024 pages is not big enough. This problem is indicated by the &lt;code&gt;WARNING&lt;/code&gt; message as well as the counts in square brackets further down in the output showing how many events were missed:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;# ./upcall_cost.py -q - Compiling eBPF programs... - Capturing events [Press ^C to stop]... ^C - Analyzing results (35486 events)... WARNING: Not all events were captured! Increase the BPF ring buffer size with the --buffer-page-count option. =&gt; Events received per type (usable/total) [missed events]: dpif_netlink_operate__op_flow_execute : 4009/ 4009 [ 45988] dpif_netlink_operate__op_flow_put : 16/ 16 [ 112] dpif_recv__recv_upcall : 7068/ 7068 [ 42928] ktrace__ovs_packet_cmd_execute : 7581/ 7581 [ 42416] openvswitch__dp_upcall : 16812/ 16812 [ 189917] ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you are losing events, increase the number of pages using the &lt;code&gt;--buffer-page-count&lt;/code&gt; option until you no longer see the &lt;code&gt;WARNING&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;# ./upcall_cost.py -q --buffer-page-count 16385 - Compiling eBPF programs... - Capturing events [Press ^C to stop]... ^C - Analyzing results (151493 events)... =&gt; Events received per type (usable/total) [missed events]: dpif_netlink_operate__op_flow_execute : 31618/ 31618 [ 0] dpif_netlink_operate__op_flow_put : 256/ 256 [ 0] dpif_recv__recv_upcall : 31618/ 31618 [ 0] ktrace__ovs_packet_cmd_execute : 31618/ 31618 [ 0] openvswitch__dp_upcall : 56383/ 56383 [ 0] ...&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;What does the upcall_cost.py output mean?&lt;/h3&gt; &lt;p&gt;To show &lt;code&gt;upcall_cost.py&lt;/code&gt; in action, this section runs the script when 10 packets arrive at wire speed with incrementing source and destination IPv4 addresses. These packets will create 10 upcalls, where each will install a new datapath flow. The following example omits the &lt;code&gt;--quiet&lt;/code&gt; option.&lt;/p&gt; &lt;p&gt;The initial output is:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./upcall_cost.py - Compiling eBPF programs... - Capturing events [Press ^C to stop]... EVENT COMM PID CPU TIME EVENT DATA[dpif_name/dp_port/pkt_len/pkt_frag_len] [openvswitch__dp_upcall] swapper/8 0 [008] 1802346.842674263: ovs-system 2 60 0 [openvswitch__dp_upcall] swapper/24 0 [024] 1802346.842674671: ovs-system 2 60 0 [openvswitch__dp_upcall] swapper/22 0 [022] 1802346.842674792: ovs-system 2 60 0 [openvswitch__dp_upcall] swapper/13 0 [013] 1802346.842674989: ovs-system 2 60 0 [openvswitch__dp_upcall] swapper/0 0 [000] 1802346.842675215: ovs-system 2 60 0 [openvswitch__dp_upcall] swapper/8 0 [008] 1802346.842685418: ovs-system 2 60 0 [openvswitch__dp_upcall] swapper/22 0 [022] 1802346.842686394: ovs-system 2 60 0 [openvswitch__dp_upcall] swapper/8 0 [008] 1802346.842688965: ovs-system 2 60 0 [openvswitch__dp_upcall] swapper/8 0 [008] 1802346.842692699: ovs-system 2 60 0 [dpif_recv__recv_upcall] handler10 409283 [024] 1802346.842702062: ovs-system 2 60 [dpif_recv__recv_upcall] handler8 409281 [009] 1802346.842702155: ovs-system 2 60 [dpif_recv__recv_upcall] handler15 409288 [014] 1802346.842703571: ovs-system 2 60 [dpif_recv__recv_upcall] handler1 409274 [022] 1802346.842710813: ovs-system 2 60 [dpif_recv__recv_upcall] handler8 409281 [009] 1802346.842732671: ovs-system 2 60 [dpif_recv__recv_upcall] handler10 409283 [024] 1802346.842734368: ovs-system 2 60 [..operate__op_flow_put] handler1 409274 [022] 1802346.842736406 [..ate__op_flow_execute] handler1 409274 [022] 1802346.842737759: 60 [dpif_recv__recv_upcall] handler10 409283 [024] 1802346.842741405: ovs-system 2 60 [..operate__op_flow_put] handler8 409281 [009] 1802346.842741889 [..ate__op_flow_execute] handler8 409281 [009] 1802346.842743288: 60 [..operate__op_flow_put] handler8 409281 [009] 1802346.842744921 [..operate__op_flow_put] handler15 409288 [014] 1802346.842745244 [..ate__op_flow_execute] handler8 409281 [009] 1802346.842745913: 60 [..ate__op_flow_execute] handler15 409288 [014] 1802346.842746802: 60 [dpif_recv__recv_upcall] handler10 409283 [024] 1802346.842747036: ovs-system 2 60 [dpif_recv__recv_upcall] handler10 409283 [024] 1802346.842752903: ovs-system 2 60 [..s_packet_cmd_execute] handler1 409274 [022] 1802346.842757603 [..s_packet_cmd_execute] handler15 409288 [014] 1802346.842758324 [..s_packet_cmd_execute] handler8 409281 [009] 1802346.842759437 [..operate__op_flow_put] handler10 409283 [024] 1802346.842761813 [..ate__op_flow_execute] handler10 409283 [024] 1802346.842763171: 60 [..operate__op_flow_put] handler10 409283 [024] 1802346.842764504 [..s_packet_cmd_execute] handler8 409281 [009] 1802346.842765386 [..ate__op_flow_execute] handler10 409283 [024] 1802346.842765481: 60 [..operate__op_flow_put] handler10 409283 [024] 1802346.842766751 [..ate__op_flow_execute] handler10 409283 [024] 1802346.842767685: 60 [..operate__op_flow_put] handler10 409283 [024] 1802346.842768766 [..ate__op_flow_execute] handler10 409283 [024] 1802346.842769672: 60 [..operate__op_flow_put] handler10 409283 [024] 1802346.842770857 [..ate__op_flow_execute] handler10 409283 [024] 1802346.842771819: 60 [..s_packet_cmd_execute] handler10 409283 [024] 1802346.842784813 [..s_packet_cmd_execute] handler10 409283 [024] 1802346.842790567 [..s_packet_cmd_execute] handler10 409283 [024] 1802346.842793300 [..s_packet_cmd_execute] handler10 409283 [024] 1802346.842795959 [..s_packet_cmd_execute] handler10 409283 [024] 1802346.842798544 [openvswitch__dp_upcall] swapper/25 0 [025] 1802346.862490199: ovs-system 2 60 0 [dpif_recv__recv_upcall] handler11 409284 [011] 1802346.862515346: ovs-system 2 60 [..operate__op_flow_put] handler11 409284 [011] 1802346.862533071 [..ate__op_flow_execute] handler11 409284 [011] 1802346.862534409: 60 [..s_packet_cmd_execute] handler11 409284 [011] 1802346.862546508 ^C&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, the output shows the total number of events received and their types. This kind of output also appeared in examples in the previous section because it indicates when there are potentially missed events:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;- Analyzing results (50 events)... =&gt; Events received per type (usable/total) [missed events]: dpif_netlink_operate__op_flow_execute : 10/ 10 [ 0] dpif_netlink_operate__op_flow_put : 10/ 10 [ 0] dpif_recv__recv_upcall : 10/ 10 [ 0] ktrace__ovs_packet_cmd_execute : 10/ 10 [ 0] openvswitch__dp_upcall : 10/ 10 [ 0]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, the script starts correlating events. The following example shows 10 sets of correlated events, which is as expected, because we have sent 10 packets without a datapath flow, so they needed full processing:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;- Matching DP_UPCALLs to RECV_UPCALLs |████████████████████████████████████████| 10/10 [100%] in 0.0s (478.15/s) - Analyzing 10 event sets...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next are statistics related to which handler threads processed the upcall by calling &lt;code&gt;dpif_recv()&lt;/code&gt; and reading the data from the socket. These statistics might be interesting to see whether the upcalls are distributed evenly across threads. If not, this might be an interesting area for research. This imbalance could be perfectly fine because you might only have a limited set of flows, but it could also be the sign of a bug or misconfiguration:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;=&gt; Upcalls handled per thread: handler10 : 5 handler8 : 2 handler15 : 1 handler1 : 1 handler11 : 1&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Batching and event types&lt;/h3&gt; &lt;p&gt;Before looking at the next set of histograms, we need to talk about the events being captured and their relationships. We'll look at five types of events, two executed in the kernel and three by OVS through Userland Statically Defined Tracing (USDT) probes.&lt;/p&gt; &lt;p&gt;The events generated within the kernel are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;openvswitch__dp_upcall&lt;/code&gt;: A kernel tracepoint, which appears every time the OVS kernel module wants to send a packet to userspace.&lt;/li&gt; &lt;li&gt;&lt;code&gt;ktrace__ovs_packet_cmd_execute&lt;/code&gt;: A &lt;code&gt;kprobe&lt;/code&gt;, which appears every time the OVS kernel module receives a packet from userspace that it needs to execute, i.e. forward.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The events generated by USDT probes in OVS are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;dpif_recv__recv_upcall&lt;/code&gt;: An event that appears every time an upcall is read from the kernel.&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_netlink_operate__op_flow_put&lt;/code&gt;: An event that appears every time the Netlink datapath is about to execute the &lt;code&gt;DPIF_OP_FLOW_PUT&lt;/code&gt; operation as part of the &lt;code&gt;dpif_operate()&lt;/code&gt; callback.&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_netlink_operate__op_flow_execute&lt;/code&gt;: An event that appears every time the Netlink datapath is about to execute the &lt;code&gt;DPIF_OP_FLOW_EXECUTE&lt;/code&gt; operation as part of the &lt;code&gt;dpif_operate()&lt;/code&gt; callback.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;OVS usually reads upcall messages in the &lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; function in batches, with a maximum of 64 messages per batch. When OVS doesn't batch the upcalls, the sequence of events is:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;dpif_recv__recv_upcall&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_netlink_operate__op_flow_put&lt;/code&gt; &lt;sup&gt;1&lt;/sup&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpof_netlink_operate__op_flow_execute&lt;/code&gt; &lt;sup&gt;2&lt;/sup&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;ktrace__ovs_packet_cmd_execute&lt;/code&gt; &lt;sup&gt;2&lt;/sup&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Note that the flow put operation, &lt;code&gt;dpif_netlink_operate__op_flow_put&lt;/code&gt;, might be missing if the datapath flow was already pushed to the kernel. This can happen when two or more packets of the same flow are queued for slow-path processing.&lt;/p&gt; &lt;p&gt;Also note that the flow execution operations, &lt;code&gt;dpof_netlink_operate__op_flow_execute&lt;/code&gt; and &lt;code&gt;ace__ovs_packet_cmd_execute&lt;/code&gt;,  might be missing if, for example, the packet was sent up for forwarding to the controller.&lt;/p&gt; &lt;p&gt;More often, OVS batches the received upcalls. To clarify the sequence in this case, we take the example of three upcalls called A, B, and C:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; &lt;strong&gt;A&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; &lt;strong&gt;B&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; &lt;strong&gt;C&lt;/strong&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;dpif_recv__recv_upcall&lt;/code&gt; &lt;strong&gt;A&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_recv__recv_upcall&lt;/code&gt; &lt;strong&gt;B&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_recv__recv_upcall&lt;/code&gt; &lt;strong&gt;C&lt;/strong&gt; &lt;ul&gt;&lt;li&gt;&lt;em&gt;OVS is doing all the flow lookups/preprocessing for A, B, and C.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_netlink_operate__op_flow_put&lt;/code&gt; &lt;strong&gt;A&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_netlink_operate__op_flow_execute&lt;/code&gt; &lt;strong&gt;A&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_netlink_operate__op_flow_put&lt;/code&gt; &lt;strong&gt;B&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_netlink_operate__op_flow_execute&lt;/code&gt; &lt;strong&gt;B&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_netlink_operate__op_flow_put&lt;/code&gt; &lt;strong&gt;C&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;dpif_netlink_operate__op_flow_execute&lt;/code&gt; &lt;strong&gt;C&lt;/strong&gt; &lt;ul&gt;&lt;li&gt;&lt;em&gt;OVS sends the batched netlink message to the kernel.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;ktrace__ovs_packet_cmd_execute&lt;/code&gt; &lt;strong&gt;A&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;ktrace__ovs_packet_cmd_execute&lt;/code&gt; &lt;strong&gt;B&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;ktrace__ovs_packet_cmd_execute&lt;/code&gt; &lt;strong&gt;C&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Histograms&lt;/h3&gt; &lt;p&gt;Now that we know the sequence of events, let's discuss the output of the first histogram. This one is straightforward: It shows the total number of sets (the total number of sequences for a single packet, as explained earlier), the minimum batch size, and the maximum batch size:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Histogram of upcalls per batch: # NumSamples = 5; Min = 1; Max = 5 # each ∎ represents a count of 1 1 [ 3]: ∎∎∎ 33 [ 0]: 2 [ 1]: ∎ 34 [ 0]: 3 [ 0]: 35 [ 0]: 4 [ 0]: 36 [ 0]: 5 [ 1]: ∎ 37 [ 0]: 6 [ 0]: 38 [ 0]: 7 [ 0]: 39 [ 0]: 8 [ 0]: 40 [ 0]: 9 [ 0]: 41 [ 0]: 10 [ 0]: 42 [ 0]: 11 [ 0]: 43 [ 0]: 12 [ 0]: 44 [ 0]: 13 [ 0]: 45 [ 0]: 14 [ 0]: 46 [ 0]: 15 [ 0]: 47 [ 0]: 16 [ 0]: 48 [ 0]: 17 [ 0]: 49 [ 0]: 18 [ 0]: 50 [ 0]: 19 [ 0]: 51 [ 0]: 20 [ 0]: 52 [ 0]: 21 [ 0]: 53 [ 0]: 22 [ 0]: 54 [ 0]: 23 [ 0]: 55 [ 0]: 24 [ 0]: 56 [ 0]: 25 [ 0]: 57 [ 0]: 26 [ 0]: 58 [ 0]: 27 [ 0]: 59 [ 0]: 28 [ 0]: 60 [ 0]: 29 [ 0]: 61 [ 0]: 30 [ 0]: 62 [ 0]: 31 [ 0]: 63 [ 0]: 32 [ 0]: 64 [ 0]:&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The next four histograms have a slightly different format, but still show the number of samples (sets) and the minimum and maximum value as before. In addition, the output shows some statistical properties: mean, variance, standard deviation (SD), and median. All times are in microseconds.&lt;/p&gt; &lt;p&gt;The first histogram we'll examine shows the delta between the &lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; and &lt;code&gt;dpif_recv__recv_upcall&lt;/code&gt; events: In other words, the time it takes from the kernel queueing the upcall data till the &lt;code&gt;ovs-vswitchd&lt;/code&gt; reads it from the Netlink socket:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Kernel upcall action to vswitchd receive (microseconds): # NumSamples = 10; Min = 25.15; Max = 60.20 # Mean = 42.472500; Variance = 206.186660; SD = 14.359201; Median 40.937500 # each ∎ represents a count of 1 25.1470 - 26.8998 [ 1]: ∎ 26.8998 - 28.6527 [ 3]: ∎∎∎ 28.6527 - 30.4055 [ 0]: 30.4055 - 32.1584 [ 0]: 32.1584 - 33.9112 [ 0]: 33.9112 - 35.6641 [ 1]: ∎ 35.6641 - 37.4169 [ 0]: 37.4169 - 39.1698 [ 0]: 39.1698 - 40.9226 [ 0]: 40.9226 - 42.6755 [ 0]: 42.6755 - 44.4283 [ 0]: 44.4283 - 46.1812 [ 0]: 46.1812 - 47.9340 [ 1]: ∎ 47.9340 - 49.6869 [ 0]: 49.6869 - 51.4398 [ 0]: 51.4398 - 53.1926 [ 0]: 53.1926 - 54.9454 [ 0]: 54.9454 - 56.6983 [ 1]: ∎ 56.6983 - 58.4511 [ 1]: ∎ 58.4511 - 60.2040 [ 2]: ∎∎&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following histogram shows the reverse of the previous one, meaning the delta between the &lt;code&gt;dpif_netlink_operate__op_flow_execute&lt;/code&gt; and &lt;code&gt;ktrace__ovs_packet_cmd_execute&lt;/code&gt; events: In other words, the time it takes from the &lt;code&gt;ovs-vswitchd&lt;/code&gt; queuing the &lt;code&gt;DPIF_OP_FLOW_EXECUTE&lt;/code&gt; operation till the OVS kernel module receives it:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; vswitchd execute to kernel receive (microseconds): # NumSamples = 10; Min = 11.52; Max = 26.73 # Mean = 20.444200; Variance = 29.230519; SD = 5.406526; Median 20.743000 # each ∎ represents a count of 1 11.5220 - 12.2821 [ 2]: ∎∎ 12.2821 - 13.0423 [ 0]: 13.0423 - 13.8025 [ 0]: 13.8025 - 14.5626 [ 0]: 14.5626 - 15.3228 [ 0]: 15.3228 - 16.0829 [ 0]: 16.0829 - 16.8431 [ 1]: ∎ 16.8431 - 17.6032 [ 0]: 17.6032 - 18.3634 [ 0]: 18.3634 - 19.1235 [ 0]: 19.1235 - 19.8837 [ 2]: ∎∎ 19.8837 - 20.6438 [ 0]: 20.6438 - 21.4040 [ 0]: 21.4040 - 22.1641 [ 1]: ∎ 22.1641 - 22.9243 [ 0]: 22.9243 - 23.6844 [ 0]: 23.6844 - 24.4445 [ 0]: 24.4445 - 25.2047 [ 1]: ∎ 25.2047 - 25.9649 [ 1]: ∎ 25.9649 - 26.7250 [ 2]: ∎∎&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following histogram shows the total time it takes from the &lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; event till the &lt;code&gt;ktrace__ovs_packet_cmd_execute&lt;/code&gt; event, minus the time &lt;code&gt;ovs-vswitchd&lt;/code&gt; spends doing the actual flow lookup/preparation. Put mathematically, the value reflects:&lt;/p&gt; &lt;p&gt;ΔT=(&lt;code&gt;ktrace__ovs_packet_cmd_execute&lt;/code&gt; − &lt;code&gt;openvswitch__dp_upcall&lt;/code&gt;) - (&lt;code&gt;dpif_netlink_operate__op_flow_put&lt;/code&gt; − &lt;code&gt;dpif_recv__recv_upcall&lt;/code&gt;)&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Upcall overhead (total time minus lookup) (microseconds): # NumSamples = 10; Min = 38.58; Max = 87.89 # Mean = 64.094400; Variance = 361.183667; SD = 19.004833; Median 61.768500 # each ∎ represents a count of 1 38.5840 - 41.0494 [ 1]: ∎ 41.0494 - 43.5147 [ 1]: ∎ 43.5147 - 45.9801 [ 1]: ∎ 45.9801 - 48.4454 [ 0]: 48.4454 - 50.9108 [ 1]: ∎ 50.9108 - 53.3761 [ 0]: 53.3761 - 55.8415 [ 0]: 55.8415 - 58.3068 [ 1]: ∎ 58.3068 - 60.7722 [ 0]: 60.7722 - 63.2375 [ 0]: 63.2375 - 65.7028 [ 0]: 65.7028 - 68.1682 [ 1]: ∎ 68.1682 - 70.6335 [ 0]: 70.6335 - 73.0989 [ 0]: 73.0989 - 75.5643 [ 0]: 75.5643 - 78.0296 [ 0]: 78.0296 - 80.4950 [ 0]: 80.4950 - 82.9603 [ 1]: ∎ 82.9603 - 85.4257 [ 1]: ∎ 85.4257 - 87.8910 [ 2]: ∎∎&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The final histogram shows the total time it takes from the &lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; event till the &lt;code&gt;ktrace__ovs_packet_cmd_execute&lt;/code&gt; event:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Kernel upcall to kernel packet execute (microseconds): # NumSamples = 10; Min = 56.31; Max = 116.30 # Mean = 93.283600; Variance = 323.670661; SD = 17.990849; Median 95.245000 # each ∎ represents a count of 1 56.3090 - 59.3087 [ 1]: ∎ 59.3087 - 62.3085 [ 0]: 62.3085 - 65.3083 [ 0]: 65.3083 - 68.3080 [ 0]: 68.3080 - 71.3077 [ 0]: 71.3077 - 74.3075 [ 0]: 74.3075 - 77.3072 [ 0]: 77.3072 - 80.3070 [ 1]: ∎ 80.3070 - 83.3067 [ 1]: ∎ 83.3067 - 86.3065 [ 2]: ∎∎ 86.3065 - 89.3063 [ 0]: 89.3063 - 92.3060 [ 0]: 92.3060 - 95.3057 [ 0]: 95.3057 - 98.3055 [ 0]: 98.3055 - 101.3053 [ 0]: 101.3053 - 104.3050 [ 0]: 104.3050 - 107.3047 [ 2]: ∎∎ 107.3047 - 110.3045 [ 2]: ∎∎ 110.3045 - 113.3042 [ 0]: 113.3042 - 116.3040 [ 1]: ∎&lt;/code&gt;&lt;/pre&gt; &lt;h2 id="getting-some-real-world-data"&gt;Runs with real-world data&lt;/h2&gt; &lt;p&gt;The data we will present next was taken on a node within a 16-node OpenShift cluster running &lt;a href="https://www.ovn.org/"&gt;Open Virtual Network (OVN)&lt;/a&gt; on top of OVS, over the duration of two hours and 20 minutes. The goal was to simulate a real-world traffic set, using both TCP and UDP with a mix of streamed and request-response-like traffic.&lt;/p&gt; &lt;p&gt;Before we started the script, we made sure all pods needed in the cluster were up and running, so no OVS port modifications were required. (Some modifications did happen during the run, but those events were ignored).&lt;/p&gt; &lt;p&gt;Let's dump the output of the run and add some remarks to it:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# ./upcall_cost.py -q -r ovn_ocp_stats.dat - Reading events from "ovn_ocp_stats.dat"... - Analyzing results (309351 events)... =&gt; Events received per type (usable/total) [missed events]: dpif_netlink_operate__op_flow_execute : 65294/ 65294 [ 0] dpif_netlink_operate__op_flow_put : 56135/ 56135 [ 0] dpif_recv__recv_upcall : 64969/ 64969 [ 0] ktrace__ovs_packet_cmd_execute : 65292/ 65292 [ 0] openvswitch__dp_upcall : 57661/ 64224 [ 0] - Matching DP_UPCALLs to RECV_UPCALLs |████████████████████████████████████████| 57661/57661 [100%] in 10.1s (5683.50/s) WARNING: SKB from kernel had fragments, we could only copy/compare the first part! - Analyzing 57538 event sets...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first thing to notice is that the upcalls are not nicely distributed among the handler threads. There could be all kinds of reasons, which might be worth investigating. Because the saved events have a lot of data, including a packet fragment, it's possible to write an additional script the analyze the data:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;=&gt; Upcalls handled per thread: handler1 : 26769 handler2 : 21067 handler5 : 5148 handler6 : 2665 handler4 : 9320&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Second, the batch size is only 9, so there is probably not a high rate of upcalls coming in:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Histogram of upcalls per batch: # NumSamples = 61069; Min = 1; Max = 9 # each ∎ represents a count of 14575 1 [ 58301]: ∎∎∎∎ 33 [ 0]: 2 [ 2052]: 34 [ 0]: 3 [ 451]: 35 [ 0]: 4 [ 167]: 36 [ 0]: 5 [ 62]: 37 [ 0]: 6 [ 23]: 38 [ 0]: 7 [ 10]: 39 [ 0]: 8 [ 2]: 40 [ 0]: 9 [ 1]: 41 [ 0]: 10 [ 0]: 42 [ 0]: 11 [ 0]: 43 [ 0]: 12 [ 0]: 44 [ 0]: 13 [ 0]: 45 [ 0]: 14 [ 0]: 46 [ 0]: 15 [ 0]: 47 [ 0]: 16 [ 0]: 48 [ 0]: 17 [ 0]: 49 [ 0]: 18 [ 0]: 50 [ 0]: 19 [ 0]: 51 [ 0]: 20 [ 0]: 52 [ 0]: 21 [ 0]: 53 [ 0]: 22 [ 0]: 54 [ 0]: 23 [ 0]: 55 [ 0]: 24 [ 0]: 56 [ 0]: 25 [ 0]: 57 [ 0]: 26 [ 0]: 58 [ 0]: 27 [ 0]: 59 [ 0]: 28 [ 0]: 60 [ 0]: 29 [ 0]: 61 [ 0]: 30 [ 0]: 62 [ 0]: 31 [ 0]: 63 [ 0]: 32 [ 0]: 64 [ 0]:&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following four histograms show various measurements of time spent processing various parts of the upcalls. The last two show the difference between the total time and the total time minus the flow lookup time.&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Kernel upcall action to vswitchd receive (microseconds): # NumSamples = 57538; Min = 5.28; Max = 1017.34 # Mean = 37.127455; Variance = 449.345903; SD = 21.197781; Median 28.918000 # each ∎ represents a count of 591 5.2830 - 55.8858 [ 44328]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 55.8858 - 106.4885 [ 13091]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 106.4885 - 157.0912 [ 89]: 157.0912 - 207.6940 [ 7]: 207.6940 - 258.2968 [ 5]: 258.2968 - 308.8995 [ 3]: 308.8995 - 359.5023 [ 5]: 359.5023 - 410.1050 [ 2]: 410.1050 - 460.7078 [ 4]: 460.7078 - 511.3105 [ 0]: 511.3105 - 561.9133 [ 0]: 561.9133 - 612.5160 [ 0]: 612.5160 - 663.1187 [ 0]: 663.1187 - 713.7215 [ 0]: 713.7215 - 764.3243 [ 1]: 764.3243 - 814.9270 [ 0]: 814.9270 - 865.5298 [ 2]: 865.5298 - 916.1325 [ 0]: 916.1325 - 966.7353 [ 0]: 966.7353 - 1017.3380 [ 1]:&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; vswitchd execute to kernel receive (microseconds): # NumSamples = 56446; Min = 2.34; Max = 991.14 # Mean = 25.006876; Variance = 186.347754; SD = 13.650925; Median 24.358000 # each ∎ represents a count of 732 2.3440 - 51.7839 [ 54970]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 51.7839 - 101.2239 [ 1394]: ∎ 101.2239 - 150.6638 [ 71]: 150.6638 - 200.1038 [ 5]: 200.1038 - 249.5437 [ 2]: 249.5437 - 298.9837 [ 0]: 298.9837 - 348.4236 [ 0]: 348.4236 - 397.8636 [ 0]: 397.8636 - 447.3035 [ 0]: 447.3035 - 496.7435 [ 0]: 496.7435 - 546.1834 [ 1]: 546.1834 - 595.6234 [ 0]: 595.6234 - 645.0634 [ 0]: 645.0634 - 694.5033 [ 0]: 694.5033 - 743.9433 [ 0]: 743.9433 - 793.3832 [ 0]: 793.3832 - 842.8231 [ 0]: 842.8231 - 892.2631 [ 0]: 892.2631 - 941.7030 [ 1]: 941.7030 - 991.1430 [ 2]:&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Upcall overhead (total time minus lookup) (microseconds): # NumSamples = 51329; Min = 18.45; Max = 1051.58 # Mean = 66.283183; Variance = 748.900766; SD = 27.366051; Median 58.436000 # each ∎ represents a count of 442 18.4520 - 70.1084 [ 33178]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 70.1084 - 121.7647 [ 17153]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 121.7647 - 173.4210 [ 927]: ∎∎ 173.4210 - 225.0774 [ 50]: 225.0774 - 276.7337 [ 6]: 276.7337 - 328.3901 [ 1]: 328.3901 - 380.0464 [ 2]: 380.0464 - 431.7028 [ 1]: 431.7028 - 483.3591 [ 2]: 483.3591 - 535.0155 [ 1]: 535.0155 - 586.6718 [ 1]: 586.6718 - 638.3282 [ 0]: 638.3282 - 689.9845 [ 0]: 689.9845 - 741.6409 [ 0]: 741.6409 - 793.2972 [ 1]: 793.2972 - 844.9536 [ 0]: 844.9536 - 896.6099 [ 1]: 896.6099 - 948.2663 [ 1]: 948.2663 - 999.9226 [ 2]: 999.9226 - 1051.5790 [ 2]:&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Kernel upcall to kernel packet execute (microseconds): # NumSamples = 56446; Min = 19.57; Max = 1306.49 # Mean = 149.825728; Variance = 4282.641028; SD = 65.441890; Median 144.253500 # each ∎ represents a count of 276 19.5700 - 83.9161 [ 8637]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 83.9161 - 148.2623 [ 20707]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 148.2623 - 212.6084 [ 18989]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 212.6084 - 276.9546 [ 6135]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 276.9546 - 341.3007 [ 1431]: ∎∎∎∎∎ 341.3007 - 405.6469 [ 380]: ∎ 405.6469 - 469.9930 [ 101]: 469.9930 - 534.3392 [ 34]: 534.3392 - 598.6853 [ 14]: 598.6853 - 663.0315 [ 6]: 663.0315 - 727.3777 [ 2]: 727.3777 - 791.7238 [ 0]: 791.7238 - 856.0699 [ 2]: 856.0699 - 920.4161 [ 2]: 920.4161 - 984.7622 [ 1]: 984.7622 - 1049.1084 [ 1]: 1049.1084 - 1113.4545 [ 2]: 1113.4545 - 1177.8007 [ 1]: 1177.8007 - 1242.1468 [ 0]: 1242.1468 - 1306.4930 [ 0]:&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Figure 1 shows the spread of times as boxplots, with averages marked as red triangles and the medians as vertical orange lines. The values in the Total boxplot came from the "Kernel upcall to kernel packet execute (microseconds)" histogram shown earlier. The values in the Overhead boxplot came from the "Upcall overhead (total time minus lookup) (microseconds)" histogram and represent the time taken by the &lt;code&gt;dpif_*&lt;/code&gt; calls. The mean total is 150 microseconds, and the mean overhead is 66, or 44% of the total. The median is similar: 144 total and 58 overhead, or 40% of the total.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/f1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/f1.png?itok=30_yam2e" width="1280" height="480" alt="Overhead averages out to 40% to 44% of the total. See the text for details." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Overhead averages out to 40% to 44% of the total. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Although the overheads in Figure 1 look high, when you consider the small amount of absolute time required and know that the sequences could potentially do three kernel interactions, the overhead does not look too bad. Of course, there are outliers, but the script in the current form does not report any statistics on those.&lt;/p&gt; &lt;h2 id="looking-at-some-fabricated-data"&gt;Runs with test data&lt;/h2&gt; &lt;p&gt;In the lab, we do a lot of benchmarking, which in most cases does not use a realistic traffic pattern. One of those tests is physical to virtual to physical (PVP), where we send a high amount of traffic, mostly at wire speed, toward the device under test (DUT) and measure the throughput. This high rate of unknown traffic puts a lot of stress on the lookup part of OVS. One catch is that packets are queued to userspace until the flow gets installed, which at wire speed leads to buffer overruns and in turn lost packets.&lt;/p&gt; &lt;p&gt;Here is an example of running an &lt;a href="https://github.com/chaudron/ovs_perf"&gt;ovs_perf&lt;/a&gt; test with 64-byte packets and a total of 100 flows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# upcall_cost.py -w pvp_64_batch64_full_20sec_random_100flows.cost -q --buffer-page-count 64000 - Compiling eBPF programs... - Capturing events [Press ^C to stop]... ^C - Analyzing results (56316 events)... =&gt; Events received per type (usable/total) [missed events]: dpif_netlink_operate__op_flow_execute : 13835/ 13835 [ 0] dpif_netlink_operate__op_flow_put : 200/ 200 [ 0] dpif_recv__recv_upcall : 13835/ 13835 [ 0] ktrace__ovs_packet_cmd_execute : 13835/ 13835 [ 0] openvswitch__dp_upcall : 14611/ 14611 [ 0]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first thing to notice here is the imbalance between the 14,611 events handled by &lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; and the 13,835 events handled by &lt;code&gt;dpif_recv__recv_upcall&lt;/code&gt; and related calls. This means that 776 upcalls failed to be sent to userspace.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: I had to modify the &lt;code&gt;ovs_pvp&lt;/code&gt; script to send random packet data, or else the packet would be matched wrongly; i.e., a missed &lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; would match to the first identical packet received by &lt;code&gt;dpif_recv__recv_upcall&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Further output shows that 98% of the packets were batched together in batches of 64 packets:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;- Analyzing 13807 event sets... ... =&gt; Histogram of upcalls per batch: # NumSamples = 225; Min = 1; Max = 64 # each ∎ represents a count of 52 1 [ 2]: 33 [ 0]: 2 [ 0]: 34 [ 0]: 3 [ 0]: 35 [ 1]: 4 [ 0]: 36 [ 1]: 5 [ 0]: 37 [ 0]: 6 [ 0]: 38 [ 0]: 7 [ 0]: 39 [ 0]: 8 [ 1]: 40 [ 0]: 9 [ 0]: 41 [ 0]: 10 [ 0]: 42 [ 0]: 11 [ 0]: 43 [ 1]: 12 [ 0]: 44 [ 0]: 13 [ 0]: 45 [ 0]: 14 [ 0]: 46 [ 0]: 15 [ 0]: 47 [ 0]: 16 [ 0]: 48 [ 0]: 17 [ 1]: 49 [ 0]: 18 [ 1]: 50 [ 0]: 19 [ 1]: 51 [ 0]: 20 [ 0]: 52 [ 0]: 21 [ 0]: 53 [ 0]: 22 [ 2]: 54 [ 0]: 23 [ 0]: 55 [ 0]: 24 [ 0]: 56 [ 1]: 25 [ 0]: 57 [ 0]: 26 [ 1]: 58 [ 0]: 27 [ 1]: 59 [ 0]: 28 [ 0]: 60 [ 0]: 29 [ 0]: 61 [ 0]: 30 [ 0]: 62 [ 0]: 31 [ 0]: 63 [ 0]: 32 [ 0]: 64 [ 211]: ∎∎∎∎&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A histogram of the total time follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Kernel upcall to kernel packet execute (microseconds): # NumSamples = 13807; Min = 622.86; Max = 100147.68 # Mean = 29367.911161; Variance = 810227590.080283; SD = 28464.497011; Median 17470.188000 # each ∎ represents a count of 51 622.8620 - 5599.1031 [ 3834]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 5599.1031 - 10575.3443 [ 1209]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 10575.3443 - 15551.5854 [ 1264]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 15551.5854 - 20527.8266 [ 1036]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 20527.8266 - 25504.0678 [ 550]: ∎∎∎∎∎∎∎∎∎∎ 25504.0678 - 30480.3089 [ 640]: ∎∎∎∎∎∎∎∎∎∎∎∎ 30480.3089 - 35456.5500 [ 576]: ∎∎∎∎∎∎∎∎∎∎∎ 35456.5500 - 40432.7912 [ 576]: ∎∎∎∎∎∎∎∎∎∎∎ 40432.7912 - 45409.0324 [ 576]: ∎∎∎∎∎∎∎∎∎∎∎ 45409.0324 - 50385.2735 [ 576]: ∎∎∎∎∎∎∎∎∎∎∎ 50385.2735 - 55361.5146 [ 192]: ∎∎∎ 55361.5146 - 60337.7558 [ 315]: ∎∎∎∎∎∎ 60337.7558 - 65313.9970 [ 261]: ∎∎∎∎∎ 65313.9970 - 70290.2381 [ 384]: ∎∎∎∎∎∎∎ 70290.2381 - 75266.4792 [ 320]: ∎∎∎∎∎∎ 75266.4792 - 80242.7204 [ 256]: ∎∎∎∎∎ 80242.7204 - 85218.9615 [ 320]: ∎∎∎∎∎∎ 85218.9615 - 90195.2027 [ 320]: ∎∎∎∎∎∎ 90195.2027 - 95171.4438 [ 262]: ∎∎∎∎∎ 95171.4438 - 100147.6850 [ 340]: ∎∎∎∎∎∎&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A histogram of the same events follows with the batch size forced down to a single packet:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;=&gt; Kernel upcall to kernel packet execute (microseconds): # NumSamples = 14861; Min = 31.18; Max = 170137.33 # Mean = 41191.399403; Variance = 1889961006.419044; SD = 43473.681767; Median 21085.577000 # each ∎ represents a count of 41 31.1810 - 8536.4883 [ 3019]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 8536.4883 - 17041.7955 [ 3139]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 17041.7955 - 25547.1028 [ 1880]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 25547.1028 - 34052.4100 [ 841]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 34052.4100 - 42557.7172 [ 997]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 42557.7172 - 51063.0245 [ 937]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 51063.0245 - 59568.3317 [ 888]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ 59568.3317 - 68073.6390 [ 134]: ∎∎∎ 68073.6390 - 76578.9462 [ 331]: ∎∎∎∎∎∎∎∎ 76578.9462 - 85084.2535 [ 294]: ∎∎∎∎∎∎∎ 85084.2535 - 93589.5607 [ 233]: ∎∎∎∎∎ 93589.5607 - 102094.8680 [ 238]: ∎∎∎∎∎ 102094.8680 - 110600.1753 [ 249]: ∎∎∎∎∎∎ 110600.1753 - 119105.4825 [ 230]: ∎∎∎∎∎ 119105.4825 - 127610.7897 [ 223]: ∎∎∎∎∎ 127610.7897 - 136116.0970 [ 230]: ∎∎∎∎∎ 136116.0970 - 144621.4043 [ 273]: ∎∎∎∎∎∎ 144621.4043 - 153126.7115 [ 248]: ∎∎∎∎∎∎ 153126.7115 - 161632.0188 [ 230]: ∎∎∎∎∎ 161632.0188 - 170137.3260 [ 247]: ∎∎∎∎∎∎&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I had expected that the maximum time for a single packet batch would be less than it was for the 64-packet batch, because I removed the &lt;em&gt;relatively&lt;/em&gt; long delay of doing 64 flow lookups before the packets get sent out. However, I guess that the stress on the kernel—which these statistics don't show—could make up for the time saved.&lt;/p&gt; &lt;p&gt;Figure 2 was created using the same method as before, but instead of continuously sending packets at wire speed, we sent in 1,000 packets/flows and stopped. That is, OVS does just the learning part. In addition, we set the number of handler threads to 1. Because of the wide ranges, we represented the microseconds on the X axis exponentially. This is why the averages in the boxplots appear skewed toward the right.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/f2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/f2.png?itok=lLkZiNGM" width="1280" height="480" alt="The spread of execution times and the averages shift for different batch sizes, with medians showing very different results from means. See the text for details." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. The spread of execution times and the averages shift for different batch sizes, with medians showing very different results from means. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Smaller batch sizes enjoyed lower median execution times. In other words, 50% of the batch sizes are handled faster, but others take longer. This is probably because decreasing the receiving batch size also decreases the batching of the flow addition and execution messages toward the kernel. In addition, the total time per flow also increases over time, because we sent in more frames than the CPU can handle.&lt;/p&gt; &lt;p&gt;To alleviate this CPU load, we tried one more test that sent 100 packets, physical to virtual (PV) only. Figure 3 shows the results.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/f3.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/f3.png?itok=Gcml8ENJ" width="1280" height="480" alt="Running a stress test without extra CPU overload shows that 32-packet batches perform slightly better than 64-packet batches." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. Different times are reported by a stress test without extra CPU overload. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Whereas Figure 2 showed that a batch size of 64 offers the best mean and median performance, Figure 3 suggests that a batch size of 32 is better. These measurements include the total time it takes before all the flows are programed in the kernel module, when no upcalls are happening.&lt;/p&gt; &lt;p&gt;We could go on forever looking at more data, such as the total time it takes before all packets are handled by the kernel, the frequency and spread of the upcalls, etc. We could look at just the total time it costs to process the lookup by removing the &lt;code&gt;openvswitch__dp_upcall&lt;/code&gt; from the set. This change would probably give better results for smaller batch sizes, but output from that run of the script would not show the longer time it takes to program all flows in the system, and thus would not show the complete picture of a system under stress.&lt;/p&gt; &lt;p&gt;My sample runs have shown what I want to emphasize in this article: The way the upcalls are generated highly influences the outcome of the upcall costs.&lt;/p&gt; &lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has shown the value of measuring the cost of upcalls, giving you good insight into your system's behavior. With the &lt;code&gt;upcall_cost.py&lt;/code&gt; script available, monitoring OVS overhead is now possible on a fine-grained level.&lt;/p&gt; &lt;p&gt;Upcall costs differ between an average system and an overloaded one. It would be of interest to the community to capture and share upcall statistics. With this data, we could further tailor OVS to better behave in the most common scenarios. Try the &lt;code&gt;upcall_cost.py&lt;/code&gt; script on your systems and work with us.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/02/07/investigating-cost-open-vswitch-upcalls-linux" title="Investigating the cost of Open vSwitch upcalls in Linux"&gt;Investigating the cost of Open vSwitch upcalls in Linux&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Eelco Chaudron</dc:creator><dc:date>2022-02-07T07:00:00Z</dc:date></entry><entry><title type="html">Reverse engineer your JBoss AS-WildFly configuration to CLI</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-script/reverse-engineer-your-jboss-as-wildfly-configuration-to-cli/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-script/reverse-engineer-your-jboss-as-wildfly-configuration-to-cli/</id><updated>2022-02-07T02:12:00Z</updated><content type="html">Exporting your WildFly / JBoss EAP configuration to a CLI script is something you are going to need one day or another. No worries, the project Profile Cloner comes to rescue! (Updated to work with WildFly 26 and Java 11) I’ve cloned ProfileCloner! The original  Profile Cloner Project allows to create a clone of your ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Quarkus World Tour 2022 - The Road Never Ends</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkusworldtour-2022/&#xA;            " /><author><name>James Cobb (https://twitter.com/insectengine)</name></author><id>https://quarkus.io/blog/quarkusworldtour-2022/</id><updated>2022-02-04T00:00:00Z</updated><published>2022-02-04T00:00:00Z</published><summary type="html">For true rock stars, the road never ends. We’re bringing the band back on the road to continue bringing our unique, hands-on experience with access to Quarkus experts designed to help you get started with Java in a Kubernetes world. Whether you are a Quarkus first-timer or seasoned pro, your...</summary><dc:creator>James Cobb (https://twitter.com/insectengine)</dc:creator><dc:date>2022-02-04T00:00:00Z</dc:date></entry></feed>
